{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97183045",
   "metadata": {},
   "source": [
    "# Create and run a local RAG pipeline from scratch\n",
    "\n",
    "The goal of this notebook is to build a RAG (Retrieval Augmented Generation) pipeline from scratch and have it run on a local GPU.\n",
    "\n",
    "Specifically, we'd like to be able to open a PDF file, ask questions (queries) of it and have them answered by a Large Language Model (LLM).\n",
    "\n",
    "There are frameworks that replicate this kind of workflow, including [LlamaIndex](https://www.llamaindex.ai/) and [LangChain](https://www.langchain.com/), however, the goal of building from scratch is to be able to inspect and customize all the parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c672248d",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "\n",
    "It was introduced in the paper [*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*](https://arxiv.org/abs/2005.11401).\n",
    "\n",
    "Each step can be roughly broken down to:\n",
    "\n",
    "* **Retrieval** - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.\n",
    "* **Augmented** - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).\n",
    "* **Generation** - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f661ce",
   "metadata": {},
   "source": [
    "## Why RAG?\n",
    "\n",
    "The main goal of RAG is to improve the generation outptus of LLMs.\n",
    "\n",
    "Two primary improvements can be seen as:\n",
    "1. **Preventing hallucinations** - LLMs are incredible but they are prone to potential hallucination, as in, generating something that *looks* correct but isn't. RAG pipelines can help LLMs generate more factual outputs by providing them with factual (retrieved) inputs. And even if the generated answer from a RAG pipeline doesn't seem correct, because of retrieval, you also have access to the sources where it came from.\n",
    "2. **Work with custom data** - Many base LLMs are trained with internet-scale text data. This means they have a great ability to model language, however, they often lack specific knowledge. RAG systems can provide LLMs with domain-specific data such as medical information or company documentation and thus customized their outputs to suit specific use cases.\n",
    "\n",
    "RAG can also be a much quicker solution to implement than fine-tuning an LLM on specific data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99365f0f",
   "metadata": {},
   "source": [
    "\n",
    "## What kind of problems can RAG be used for?\n",
    "\n",
    "RAG can help anywhere there is a specific set of information that an LLM may not have in its training data (e.g. anything not publicly accessible on the internet).\n",
    "\n",
    "For example you could use RAG for:\n",
    "* **Customer support Q&A chat** - By treating your existing customer support documentation as a resource, when a customer asks a question, you could have a system retrieve relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a \"chatbot for your documentation\". Klarna, a large financial company, [uses a system like this](https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/) to save $40M per year on customer support costs.\n",
    "* **Email chain analysis** - Let's say you're an insurance company with long threads of emails between customers and insurance agents. Instead of searching through each individual email, you could retrieve relevant passages and have an LLM create strucutred outputs of insurance claims.\n",
    "* **Company internal documentation chat** - If you've worked at a large company, you know how hard it can be to get an answer sometimes. Why not let a RAG system index your company information and have an LLM answer questions you may have? The benefit of RAG is that you will have references to resources to learn more if the LLM answer doesn't suffice.\n",
    "* **Textbook Q&A** - Let's say you're studying for your exams and constantly flicking through a large textbook looking for answers to your quesitons. RAG can help provide answers as well as references to learn more.\n",
    "\n",
    "All of these have the common theme of retrieving relevant resources and then presenting them in an understandable way using an LLM.\n",
    "\n",
    "From this angle, you can consider an LLM a calculator for words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74729450",
   "metadata": {},
   "source": [
    "## Why local?\n",
    "\n",
    "Privacy, speed, cost.\n",
    "\n",
    "Running locally means you use your own hardware.\n",
    "\n",
    "From a privacy standpoint, this means you don't have send potentially sensitive data to an API.\n",
    "\n",
    "From a speed standpoint, it means you won't necessarily have to wait for an API queue or downtime, if your hardware is running, the pipeline can run.\n",
    "\n",
    "And from a cost standpoint, running on your own hardware often has a heavier starting cost but little to no costs after that.\n",
    "\n",
    "Performance wise, LLM APIs may still perform better than an open-source model running locally on general tasks but there are more and more examples appearing of smaller, focused models outperforming larger models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a1a11",
   "metadata": {},
   "source": [
    "## Key terms\n",
    "\n",
    "| Term | Description |\n",
    "| ----- | ----- | \n",
    "| **Token** | A sub-word piece of text. For example, \"hello, world!\" could be split into [\"hello\", \",\", \"world\", \"!\"]. A token can be a whole word, part of a word or group of punctuation characters. 1 token ~= 4 characters in English, 100 tokens ~= 75 words.<br> Text gets broken into tokens before being passed to an LLM. |\n",
    "| **Embedding** | A learned numerical representation of a piece of data. For example, a sentence of text could be represented by a vector with<br> 768 values. Similar pieces of text (in meaning) will ideally have similar values. |\n",
    "| **Embedding model** | A model designed to accept input data and output a numerical representation. For example, a text embedding model may take in 384 tokens of text and turn it into a vector of size 768. An embedding model can and often is different to an LLM model. |\n",
    "| **Similarity search/vector search** | Similarity search/vector search aims to find two vectors which are close together in high-demensional space. For example, two pieces of similar text passed through an embedding model should have a high similarity score, whereas two pieces of text about different topics will have a lower similarity score. Common similarity score measures are dot product and cosine similarity. |\n",
    "| **Large Language Model (LLM)** | A model which has been trained to numerically represent the patterns in text. A generative LLM will continue a sequence when given a sequence. For example, given a sequence of the text \"hello, world!\", a genertive LLM may produce \"we're going to build a RAG pipeline today!\". This generation will be highly dependant on the training data and prompt. |\n",
    "| **LLM context window** | The number of tokens a LLM can accept as input. For example, as of March 2024, GPT-4 has a default context window of 32k tokens (about 96 pages of text) but can go up to 128k if needed. A recent open-source LLM from Google, Gemma (March 2024) has a context window of 8,192 tokens (about 24 pages of text). A higher context window means an LLM can accept more relevant information to assist with a query. For example, in a RAG pipeline, if a model has a larger context window, it can accept more reference items from the retrieval system to aid with its generation. |\n",
    "| **Prompt** | A common term for describing the input to a generative LLM. The idea of \"[prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering)\" is to structure a text-based (or potentially image-based as well) input to a generative LLM in a specific way so that the generated output is ideal. This technique is possible because of a LLMs capacity for in-context learning, as in, it is able to use its representation of language to breakdown the prompt and recognize what a suitable output may be (note: the output of LLMs is probable, so terms like \"may output\" are used). | \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b06606",
   "metadata": {},
   "source": [
    " ## What we're going to build\n",
    "\n",
    "We're going to build RAG pipeline which enables us to chat with a PDF document, specifically an open-source [nutrition textbook](https://pressbooks.oer.hawaii.edu/humannutrition2/), ~1200 pages long.\n",
    "\n",
    "You could call our project NutriChat!\n",
    "\n",
    "We'll write the code to:\n",
    "1. Open a PDF document (you could use almost any PDF here).\n",
    "2. Format the text of the PDF textbook ready for an embedding model (this process is known as text splitting/chunking).\n",
    "3. Embed all of the chunks of text in the textbook and turn them into numerical representation which we can store for later.\n",
    "4. Build a retrieval system that uses vector search to find relevant chunks of text based on a query.\n",
    "5. Create a prompt that incorporates the retrieved pieces of text.\n",
    "6. Generate an answer to a query based on passages from the textbook.\n",
    "\n",
    "The above steps can broken down into two major sections:\n",
    "1. Document preprocessing/embedding creation (steps 1-3).\n",
    "2. Search and answer (steps 4-6).\n",
    "\n",
    "And that's the structure we'll follow.\n",
    "\n",
    "It's similar to the workflow outlined on the NVIDIA blog which [details a local RAG pipeline](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/).\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/simple-local-rag/blob/main/images/simple-local-rag-workflow-flowchart.png?raw=true\" alt=\"flowchart of a local RAG workflow\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64c9c9",
   "metadata": {},
   "source": [
    "## Requirements and setup\n",
    "\n",
    "* Local NVIDIA GPU (I used a NVIDIA RTX 4090 on a Windows 11 machine) or Google Colab with access to a GPU.\n",
    "* Environment setup \n",
    "* Data source (for example, a PDF). \n",
    "* Internet connection (to download the models, but once you have them, it'll run offline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbfc29",
   "metadata": {},
   "source": [
    "## 1. Document/Text Processing and Embedding Creation\n",
    "\n",
    "Ingredients:\n",
    "* PDF document of choice.\n",
    "* Embedding model of choice.\n",
    "\n",
    "Steps:\n",
    "1. Import PDF document.\n",
    "2. Process text for embedding (e.g. split into chunks of sentences).\n",
    "3. Embed text chunks with embedding model.\n",
    "4. Save embeddings to file for later use (embeddings will store on file for many years or until you lose your hard drive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede2e0b",
   "metadata": {},
   "source": [
    "### Import PDF Document \n",
    "\n",
    "This will work with many other kinds of documents.\n",
    "\n",
    "However, we'll start with PDF since many people have PDFs.\n",
    "\n",
    "But just keep in mind, text files, email chains, support documentation, articles and more can also work.\n",
    "\n",
    "We're going to pretend we're nutrition students at the University of Hawai'i, reading through the open-source PDF textbook [*Human Nutrition: 2020 Edition*](https://pressbooks.oer.hawaii.edu/humannutrition2/).\n",
    "\n",
    "There are several libraries to open PDFs with Python but I found that [PyMuPDF](https://github.com/pymupdf/pymupdf) works quite well in many cases.\n",
    "\n",
    "First we'll download the PDF if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ce88c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File human-nutrition-text.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "# Download PDF file\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Get PDF document\n",
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Download PDF if it doesn't already exist\n",
    "if not os.path.exists(pdf_path):\n",
    "  print(\"File doesn't exist, downloading...\")\n",
    "\n",
    "  # The URL of the PDF you want to download\n",
    "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "  # The local filename to save the downloaded file\n",
    "  filename = pdf_path\n",
    "\n",
    "  # Send a GET request to the URL\n",
    "  response = requests.get(url)\n",
    "\n",
    "  # Check if the request was successful\n",
    "  if response.status_code == 200:\n",
    "      # Open a file in binary write mode and save the content to it\n",
    "      with open(filename, \"wb\") as file:\n",
    "          file.write(response.content)\n",
    "      print(f\"The file has been downloaded and saved as {filename}\")\n",
    "  else:\n",
    "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "else:\n",
    "  print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736707a7",
   "metadata": {},
   "source": [
    "PDF acquired!\n",
    "\n",
    "We can import the pages of our PDF to text by first defining the PDF path and then opening and reading it with PyMuPDF (`import fitz`).\n",
    "\n",
    "We'll write a small helper function to preprocess the text as it gets read. Note that not all text will be read in the same so keep this in mind for when you prepare your text.\n",
    "\n",
    "We'll save each page to a dictionary and then append that dictionary to a list for ease of use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a6d5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7055b0fdca40df9164c292b9876f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': -41,\n",
       "  'page_char_count': 29,\n",
       "  'page_word_count': 4,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 7.25,\n",
       "  'text': 'Human Nutrition: 2020 Edition'},\n",
       " {'page_number': -40,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Open PDF and get lines/pages\n",
    "# Note: this only focuses on text, rather than images/figures etc\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f81973",
   "metadata": {},
   "source": [
    "Now let's get a random sample of the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65677f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 876,\n",
       "  'page_char_count': 815,\n",
       "  'page_word_count': 133,\n",
       "  'page_sentence_count_raw': 10,\n",
       "  'page_token_count': 203.75,\n",
       "  'text': 'Magnesium (mg)  130.0  Niacin(B3) (mg)  8.0  Phosphorus (mg)  500.0  Riboflavin (B2) (mcg)  600.0  Selenium (mcg)  30.0  Thiamine (B1) (mcg)  600.0  Zinc (mg)  5.0  Source:Institute of Medicine. 2006. Dietary Reference Intakes: The  Essential Guide to Nutrient Requirements. Washington, DC: The  National Academies Press. https://doi.org/10.17226/11537. Accessed  December 10, 2017.  Factors Influencing Intake  A number of factors can influence children’s eating habits and  attitudes toward food. Family environment, societal trends, taste  preferences, and messages in the media all impact the emotions  that children develop in relation to their diet. Television  commercials can entice children to consume sugary products, fatty  fast-foods, excess calories, refined ingredients, and sodium.  876  |  Childhood'},\n",
       " {'page_number': 308,\n",
       "  'page_char_count': 1567,\n",
       "  'page_word_count': 289,\n",
       "  'page_sentence_count_raw': 19,\n",
       "  'page_token_count': 391.75,\n",
       "  'text': 'fatty acids found in fish oil are proven to reduce the rate of  weight gain as compared to other fatty acids.1  Degrees of Saturation  Fatty acid chains are held together by carbon atoms that attach  to each other and to hydrogen atoms. The term saturation refers  to whether or not a fatty acid chain is filled (or “saturated”) to  capacity with hydrogen atoms. If each available carbon bond holds  a hydrogen atom we call this a saturated fatty acid chain. All carbon  atoms in such a fatty acid chain are bonded with single bonds.  Sometimes the chain has a place where hydrogen atoms are missing.  This is referred to as the point of unsaturation.  When one or more bonds between carbon atoms are a double  bond (C=C), that fatty acid is called an unsaturated fatty acid, as  it has one or more points of unsaturation. Any fatty acid that has  only one double bond is a monounsaturated fatty acid, an example  of which is olive oil (75 percent of its fat is monounsaturated).  Monounsaturated fats help regulate blood cholesterol levels,  thereby reducing the risk for heart disease and stroke. A  polyunsaturated fatty acid is a fatty acid with two or more double  bonds or two or more points of unsaturation. Soybean oil contains  high  amounts  of  polyunsaturated  fatty  acids.  Both  1. Mori T, Kondo H. (2007). Dietary fish oil upregulates  intestinal lipid metabolism and reduces body weight gain  in C57BL/6J mice. Journal of Nutrition, 137(12):2629-34.  http://www.ncbi.nlm.nih.gov/pubmed/18029475.  Accessed September 22, 2017.  308  |  How Lipids Work'},\n",
       " {'page_number': 569,\n",
       "  'page_char_count': 644,\n",
       "  'page_word_count': 147,\n",
       "  'page_sentence_count_raw': 15,\n",
       "  'page_token_count': 161.0,\n",
       "  'text': 'Food  Serving Niacin (mg) Percent Daily Value  Chicken  3 oz.  7.3  36.5  Tuna  3 oz.  8.6  43  Turkey  3 oz.  10.0  50  Salmon  3 oz.  8.5  42.5  Beef (90% lean)  3 oz.  4.4  22  Cereal (unfortified)  1 c.  5  25  Cereal (fortified)  1 c.  20  100  Peanuts  1 oz.  3.8  19  Whole wheat bread 1 slice  1.3  6.5  Coffee  8 oz.  0.5  2.5  Micronutrient Information Center: Niacin. Oregon State University,  Linus Pauling Institute. http://lpi.oregonstate.edu/mic/vitamins/ niacin. Updated in July 2013. Accessed October 22, 2017.  Pantothenic Acid (B5)  Figure 9.14 Pantothenic Acid‘s Role in the Citric Acid Cycle  Water-Soluble Vitamins  |  569'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10590aa",
   "metadata": {},
   "source": [
    "### Get some stats on the text\n",
    "\n",
    "Let's perform a rough exploratory data analysis (EDA) to get an idea of the size of the texts (e.g. character counts, word counts etc) we're working with.\n",
    "\n",
    "The different sizes of texts will be a good indicator into how we should split our texts.\n",
    "\n",
    "Many embedding models have limits on the size of texts they can ingest, for example, the [`sentence-transformers`](https://www.sbert.net/docs/pretrained_models.html) model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) has an input size of 384 tokens.\n",
    "\n",
    "This means that the model has been trained in ingest and turn into embeddings texts with 384 tokens (1 token ~= 4 characters ~= 0.75 words).\n",
    "\n",
    "Texts over 384 tokens which are encoded by this model will be auotmatically reduced to 384 tokens in length, potentially losing some information.\n",
    "\n",
    "We'll discuss this more in the embedding section.\n",
    "\n",
    "For now, let's turn our list of dictionaries into a DataFrame and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e34667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-39</td>\n",
       "      <td>320</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>80.00</td>\n",
       "      <td>Human Nutrition: 2020  Edition  UNIVERSITY OF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-38</td>\n",
       "      <td>212</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>53.00</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37</td>\n",
       "      <td>797</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>199.25</td>\n",
       "      <td>Contents  Preface  University of Hawai‘i at Mā...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0          -41               29                4                        1   \n",
       "1          -40                0                1                        1   \n",
       "2          -39              320               54                        1   \n",
       "3          -38              212               32                        1   \n",
       "4          -37              797              147                        3   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0              7.25                      Human Nutrition: 2020 Edition  \n",
       "1              0.00                                                     \n",
       "2             80.00  Human Nutrition: 2020  Edition  UNIVERSITY OF ...  \n",
       "3             53.00  Human Nutrition: 2020 Edition by University of...  \n",
       "4            199.25  Contents  Preface  University of Hawai‘i at Mā...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d3f9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.00</td>\n",
       "      <td>199.50</td>\n",
       "      <td>10.52</td>\n",
       "      <td>287.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.38</td>\n",
       "      <td>95.83</td>\n",
       "      <td>6.55</td>\n",
       "      <td>140.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.00</td>\n",
       "      <td>134.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>190.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1231.50</td>\n",
       "      <td>216.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>307.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1603.50</td>\n",
       "      <td>272.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>400.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>430.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>577.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1208.00          1208.00          1208.00                  1208.00   \n",
       "mean        562.50          1148.00           199.50                    10.52   \n",
       "std         348.86           560.38            95.83                     6.55   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%         260.75           762.00           134.00                     5.00   \n",
       "50%         562.50          1231.50           216.00                    10.00   \n",
       "75%         864.25          1603.50           272.00                    15.00   \n",
       "max        1166.00          2308.00           430.00                    39.00   \n",
       "\n",
       "       page_token_count  \n",
       "count           1208.00  \n",
       "mean             287.00  \n",
       "std              140.10  \n",
       "min                0.00  \n",
       "25%              190.50  \n",
       "50%              307.88  \n",
       "75%              400.88  \n",
       "max              577.00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae60f86",
   "metadata": {},
   "source": [
    "Okay, looks like our average token count per page is 287.\n",
    "\n",
    "For this particular use case, it means we could embed an average whole page with the `all-mpnet-base-v2` model (this model has an input capacity of 384)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5706da5d",
   "metadata": {},
   "source": [
    "### Further text processing (splitting pages into sentences)\n",
    "\n",
    "The ideal way of processing text before embedding it is still an active area of research.\n",
    "\n",
    "A simple method I've found helpful is to break the text into chunks of sentences.\n",
    "\n",
    "As in, chunk a page of text into groups of 5, 7, 10 or more sentences (these values are not set in stone and can be explored).\n",
    "\n",
    "But we want to follow the workflow of:\n",
    "\n",
    "`Ingest text -> split it into groups/chunks -> embed the groups/chunks -> use the embeddings`\n",
    "\n",
    "Some options for splitting text into sentences:\n",
    "\n",
    "1. Split into sentences with simple rules (e.g. split on \". \" with `text = text.split(\". \")`, like we did above).\n",
    "2. Split into sentences with a natural language processing (NLP) library such as [spaCy](https://spacy.io/) or [nltk](https://www.nltk.org/).\n",
    "\n",
    "Why split into sentences?\n",
    "\n",
    "* Easier to handle than larger pages of text (especially if pages are densely filled with text).\n",
    "* Can get specific and find out which group of sentences were used to help within a RAG pipeline.\n",
    "\n",
    "Let's use spaCy to break our text into sentences since it's likely a bit more robust than just using `text.split(\". \")`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4835b2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This is another sentence.]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/ \n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create a document instance as an example\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "assert len(list(doc.sents)) == 2\n",
    "\n",
    "# Access the sentences of the document\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabfdd76",
   "metadata": {},
   "source": [
    "We don't necessarily need to use spaCy, however, it's an open-source library designed to do NLP tasks like this at scale.\n",
    "\n",
    "So let's run our small sentencizing pipeline on our pages of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15fc40a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b89983bb1cd4f489d79af81150e1535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"]=list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "    # Make sure all sentences are strings\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    \n",
    "    # Count the sentences \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d58c278b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1134,\n",
       "  'page_char_count': 1648,\n",
       "  'page_word_count': 264,\n",
       "  'page_sentence_count_raw': 22,\n",
       "  'page_token_count': 412.0,\n",
       "  'text': 'Food Insecurity  Food insecurity is defined as not having adequate access to food  that meets nutritional needs. According to the USDA, about 48.8  million people live in food-insecure households and have reported  multiple indications of food access problems. About sixteen million  of those have “very low food security,” which means one or more  people in the household were hungry at some point over the course  of a year due to the inability to afford enough food. The difference  between low and very low food security is that members of low  insecurity households have reported problems of food access, but  have reported only a few instances of reduced food intake, if any.2  African American and Hispanic households experience food  insecurity at much higher rates than the national average.3  Households with limited resources employ a variety of methods to  increase their access to adequate food. Some families purchase junk  food and fast food—cheaper options that are also very unhealthy.  Other families who struggle with food security supplement the  groceries they purchase by participating in government assistance  2. Coleman-Jensen A. Household Food Security in the  United States in 2010. US Department of Agriculture,  Economic Research Report, no. ERR-125. 2011.   https://www.ers.usda.gov/publications/pub- details/?pubid=44909. Accessed April 15, 2018.  3. Coleman-Jensen A. Household Food Security in the  United States in 2010. US Department of Agriculture,  Economic Research Report, no. ERR-125. 2011.   https://www.ers.usda.gov/publications/pub- details/?pubid=44909. Accessed April 15, 2018.  1134  |  Food Insecurity',\n",
       "  'sentences': ['Food Insecurity  Food insecurity is defined as not having adequate access to food  that meets nutritional needs.',\n",
       "   'According to the USDA, about 48.8  million people live in food-insecure households and have reported  multiple indications of food access problems.',\n",
       "   'About sixteen million  of those have “very low food security,” which means one or more  people in the household were hungry at some point over the course  of a year due to the inability to afford enough food.',\n",
       "   'The difference  between low and very low food security is that members of low  insecurity households have reported problems of food access, but  have reported only a few instances of reduced food intake, if any.2  African American and Hispanic households experience food  insecurity at much higher rates than the national average.3  Households with limited resources employ a variety of methods to  increase their access to adequate food.',\n",
       "   'Some families purchase junk  food and fast food—cheaper options that are also very unhealthy.',\n",
       "   ' Other families who struggle with food security supplement the  groceries they purchase by participating in government assistance  2.',\n",
       "   'Coleman-Jensen A. Household Food Security in the  United States in 2010.',\n",
       "   'US Department of Agriculture,  Economic Research Report, no.',\n",
       "   'ERR-125.',\n",
       "   '2011.',\n",
       "   '  https://www.ers.usda.gov/publications/pub- details/?pubid=44909.',\n",
       "   'Accessed April 15, 2018.',\n",
       "   ' 3.',\n",
       "   'Coleman-Jensen A. Household Food Security in the  United States in 2010.',\n",
       "   'US Department of Agriculture,  Economic Research Report, no.',\n",
       "   'ERR-125.',\n",
       "   '2011.',\n",
       "   '  https://www.ers.usda.gov/publications/pub- details/?pubid=44909.',\n",
       "   'Accessed April 15, 2018.',\n",
       "   ' 1134  |  Food Insecurity'],\n",
       "  'page_sentence_count_spacy': 20}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an example\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd13228",
   "metadata": {},
   "source": [
    "Wonderful!\n",
    "\n",
    "Now let's turn out list of dictionaries into a DataFrame and get some stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa39116e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.00</td>\n",
       "      <td>199.50</td>\n",
       "      <td>10.52</td>\n",
       "      <td>287.00</td>\n",
       "      <td>10.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.38</td>\n",
       "      <td>95.83</td>\n",
       "      <td>6.55</td>\n",
       "      <td>140.10</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.00</td>\n",
       "      <td>134.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>190.50</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1231.50</td>\n",
       "      <td>216.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>307.88</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1603.50</td>\n",
       "      <td>272.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>400.88</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>430.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1208.00          1208.00          1208.00                  1208.00   \n",
       "mean        562.50          1148.00           199.50                    10.52   \n",
       "std         348.86           560.38            95.83                     6.55   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%         260.75           762.00           134.00                     5.00   \n",
       "50%         562.50          1231.50           216.00                    10.00   \n",
       "75%         864.25          1603.50           272.00                    15.00   \n",
       "max        1166.00          2308.00           430.00                    39.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count           1208.00                    1208.00  \n",
       "mean             287.00                      10.32  \n",
       "std              140.10                       6.30  \n",
       "min                0.00                       0.00  \n",
       "25%              190.50                       5.00  \n",
       "50%              307.88                      10.00  \n",
       "75%              400.88                      15.00  \n",
       "max              577.00                      28.00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a40106",
   "metadata": {},
   "source": [
    "For our set of text, it looks like our raw sentence count (e.g. splitting on `\". \"`) is quite close to what spaCy came up with.\n",
    "\n",
    "Now we've got our text split into sentences, how about we group those sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c61170",
   "metadata": {},
   "source": [
    "### Chunking our sentences together\n",
    "\n",
    "Let's take a step to break down our list of sentences/text into smaller chunks.\n",
    "\n",
    "As you might've guessed, this process is referred to as **chunking**.\n",
    "\n",
    "Why do we do this?\n",
    "\n",
    "1. Easier to manage similar sized chunks of text.\n",
    "2. Don't overload the embedding models capacity for tokens (e.g. if an embedding model has a capacity of 384 tokens, there could be information loss if you try to embed a sequence of 400+ tokens).\n",
    "3. Our LLM context window (the amount of tokens an LLM can take in) may be limited and requires compute power so we want to make sure we're using it as well as possible.\n",
    "\n",
    "Something to note is that there are many different ways emerging for creating chunks of information/text.\n",
    "\n",
    "For now, we're going to keep it simple and break our pages of sentences into groups of 10 (this number is arbitrary and can be changed, I just picked it because it seemed to line up well with our embedding model capacity of 384).\n",
    "\n",
    "On average each of our pages has 10 sentences.\n",
    "\n",
    "And an average total of 287 tokens per page.\n",
    "\n",
    "So our groups of 10 sentences will also be ~287 tokens long.\n",
    "\n",
    "This gives us plenty of room for the text to embedded by our `all-mpnet-base-v2` model (it has a capacity of 384 tokens).\n",
    "\n",
    "To split our groups of sentences into chunks of 10 or less, let's create a function which accepts a list as input and recursively breaks into down into sublists of a specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a342c720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac91cf0604644d5bb36cf6461116163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 10 \n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72fb58b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 888,\n",
       "  'page_char_count': 744,\n",
       "  'page_word_count': 123,\n",
       "  'page_sentence_count_raw': 5,\n",
       "  'page_token_count': 186.0,\n",
       "  'text': 'Learning Activities  Technology Note: The second edition of the Human  Nutrition Open Educational Resource (OER) textbook  features interactive learning activities.  These activities are  available in the web-based textbook and not available in the  downloadable versions (EPUB, Digital PDF, Print_PDF, or  Open Document).  Learning activities may be used across various mobile  devices, however, for the best user experience it is strongly  recommended that users complete these activities using a  desktop or laptop computer and in Google Chrome.    An interactive or media element has been  excluded from this version of the text. You can  view it online here:  http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=469    888  |  Adolescence',\n",
       "  'sentences': ['Learning Activities  Technology Note: The second edition of the Human  Nutrition Open Educational Resource (OER) textbook  features interactive learning activities.',\n",
       "   ' These activities are  available in the web-based textbook and not available in the  downloadable versions (EPUB, Digital PDF, Print_PDF, or  Open Document).',\n",
       "   ' Learning activities may be used across various mobile  devices, however, for the best user experience it is strongly  recommended that users complete these activities using a  desktop or laptop computer and in Google Chrome.',\n",
       "   '   An interactive or media element has been  excluded from this version of the text.',\n",
       "   'You can  view it online here:  http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=469    888  |  Adolescence'],\n",
       "  'page_sentence_count_spacy': 5,\n",
       "  'sentence_chunks': [['Learning Activities  Technology Note: The second edition of the Human  Nutrition Open Educational Resource (OER) textbook  features interactive learning activities.',\n",
       "    ' These activities are  available in the web-based textbook and not available in the  downloadable versions (EPUB, Digital PDF, Print_PDF, or  Open Document).',\n",
       "    ' Learning activities may be used across various mobile  devices, however, for the best user experience it is strongly  recommended that users complete these activities using a  desktop or laptop computer and in Google Chrome.',\n",
       "    '   An interactive or media element has been  excluded from this version of the text.',\n",
       "    'You can  view it online here:  http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=469    888  |  Adolescence']],\n",
       "  'num_chunks': 1}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample an example from the group (note: many samples have only 1 chunk as they have <=10 sentences total)\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9037107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.00</td>\n",
       "      <td>199.50</td>\n",
       "      <td>10.52</td>\n",
       "      <td>287.00</td>\n",
       "      <td>10.32</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.38</td>\n",
       "      <td>95.83</td>\n",
       "      <td>6.55</td>\n",
       "      <td>140.10</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.00</td>\n",
       "      <td>134.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>190.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1231.50</td>\n",
       "      <td>216.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>307.88</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1603.50</td>\n",
       "      <td>272.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>400.88</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>430.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1208.00          1208.00          1208.00                  1208.00   \n",
       "mean        562.50          1148.00           199.50                    10.52   \n",
       "std         348.86           560.38            95.83                     6.55   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%         260.75           762.00           134.00                     5.00   \n",
       "50%         562.50          1231.50           216.00                    10.00   \n",
       "75%         864.25          1603.50           272.00                    15.00   \n",
       "max        1166.00          2308.00           430.00                    39.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  num_chunks  \n",
       "count           1208.00                    1208.00     1208.00  \n",
       "mean             287.00                      10.32        1.53  \n",
       "std              140.10                       6.30        0.64  \n",
       "min                0.00                       0.00        0.00  \n",
       "25%              190.50                       5.00        1.00  \n",
       "50%              307.88                      10.00        1.00  \n",
       "75%              400.88                      15.00        2.00  \n",
       "max              577.00                      28.00        3.00  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to get stats\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3d1a3",
   "metadata": {},
   "source": [
    "Note how the average number of chunks is around 1.5, this is expected since many of our pages only contain an average of 10 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad5d9b",
   "metadata": {},
   "source": [
    "### Splitting each chunk into its own item\n",
    "\n",
    "We'd like to embed each chunk of sentences into its own numerical representation.\n",
    "\n",
    "So to keep things clean, let's create a new list of dictionaries each containing a single chunk of sentences with relative information such as page number as well statistics about each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f4afac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3da6ec807444b6ba9924a816ad96742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1843"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6268970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 518,\n",
       "  'sentence_chunk': 'Image by Allison Calabrese / CC BY 4.0 One major difference between fat-soluble vitamins and water- soluble vitamins is the way they are absorbed in the body. Vitamins are absorbed primarily in the small intestine and their bioavailability is dependent on the food composition of the diet. Fat-soluble vitamins are absorbed along with dietary fat. Therefore, if a meal is very low in fat, the absorption of the fat-soluble vitamins will be impaired. Once fat-soluble vitamins have been absorbed in the small intestine, they are packaged and incorporated into chylomicrons along with other fatty acids and transported in the lymphatic system to the liver. Water-soluble vitamins on the other hand are absorbed in the small intestine but are transported to the liver through blood vessels. Figure 9.2 “Absorption of Fat-Soluble and Water-Soluble Vitamins  518 | Introduction',\n",
       "  'chunk_char_count': 872,\n",
       "  'chunk_word_count': 137,\n",
       "  'chunk_token_count': 218.0}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56de4d",
   "metadata": {},
   "source": [
    "Excellent!\n",
    "\n",
    "Now we've broken our whole textbook into chunks of 10 sentences or less as well as the page number they came from.\n",
    "\n",
    "This means we could reference a chunk of text and know its source.\n",
    "\n",
    "Let's get some stats about our chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a97cd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>583.38</td>\n",
       "      <td>734.10</td>\n",
       "      <td>112.74</td>\n",
       "      <td>183.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>347.79</td>\n",
       "      <td>447.51</td>\n",
       "      <td>71.24</td>\n",
       "      <td>111.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>280.50</td>\n",
       "      <td>315.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>586.00</td>\n",
       "      <td>745.00</td>\n",
       "      <td>115.00</td>\n",
       "      <td>186.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>890.00</td>\n",
       "      <td>1118.00</td>\n",
       "      <td>173.00</td>\n",
       "      <td>279.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>1830.00</td>\n",
       "      <td>297.00</td>\n",
       "      <td>457.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count      1843.00           1843.00           1843.00            1843.00\n",
       "mean        583.38            734.10            112.74             183.52\n",
       "std         347.79            447.51             71.24             111.88\n",
       "min         -41.00             12.00              3.00               3.00\n",
       "25%         280.50            315.00             45.00              78.75\n",
       "50%         586.00            745.00            115.00             186.25\n",
       "75%         890.00           1118.00            173.00             279.50\n",
       "max        1166.00           1830.00            297.00             457.50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to get stats\n",
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a26a5b5",
   "metadata": {},
   "source": [
    "Hmm looks like some of our chunks have quite a low token count.\n",
    "\n",
    "How about we check for samples with less than 30 tokens (about the length of a sentence) and see if they are worth keeping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1834fb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 3.75 | Text: 806 | Pregnancy\n",
      "Chunk token count: 9.25 | Text: 490 | Factors Affecting Energy Intake\n",
      "Chunk token count: 24.25 | Text: There are several lecithin supplements on the market Nonessential and Essential Fatty Acids | 315\n",
      "Chunk token count: 20.75 | Text: Centers for Disease Control and Prevention.http://www.cdc.gov/nutrition/ Iron | 661\n",
      "Chunk token count: 10.5 | Text: Consequences of Deficiency or Excess | 205\n"
     ]
    }
   ],
   "source": [
    "# Show random chunks with under 30 tokens in length\n",
    "min_token_length = 30\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b91233c",
   "metadata": {},
   "source": [
    "Looks like many of these are headers and footers of different pages.\n",
    "\n",
    "They don't seem to offer too much information.\n",
    "\n",
    "Let's filter our DataFrame/list of dictionaries to only include chunks with over 30 tokens in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb21585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': -39,\n",
       "  'sentence_chunk': 'Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FIALKOWSKI REVILLA, JENNIFER DRAPER, GEMADY LANGFELDER, CHERYL GIBBY, CHYNA NICOLE CHUN, AND ALLISON CALABRESE',\n",
       "  'chunk_char_count': 308,\n",
       "  'chunk_word_count': 42,\n",
       "  'chunk_token_count': 77.0},\n",
       " {'page_number': -38,\n",
       "  'sentence_chunk': 'Human Nutrition: 2020 Edition by University of Hawai‘i at Mānoa Food Science and Human Nutrition Program is licensed under a Creative Commons Attribution 4.0 International License, except where otherwise noted.',\n",
       "  'chunk_char_count': 210,\n",
       "  'chunk_word_count': 30,\n",
       "  'chunk_token_count': 52.5}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5efea",
   "metadata": {},
   "source": [
    "Smaller chunks filtered!\n",
    "\n",
    "Time to embed our chunks of text!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b8241",
   "metadata": {},
   "source": [
    "### Embedding our text chunks\n",
    "\n",
    "While humans understand text, machines understand numbers best.\n",
    "\n",
    "An [embedding](https://vickiboykis.com/what_are_embeddings/index.html) is a broad concept.\n",
    "\n",
    "The most powerful thing about modern embeddings is that they are *learned* representations.\n",
    "\n",
    "Meaning rather than directly mapping words/tokens/characters to numbers directly (e.g. `{\"a\": 0, \"b\": 1, \"c\": 3...}`), the numerical representation of tokens is learned by going through large corpuses of text and figuring out how different tokens relate to each other.\n",
    "\n",
    "Ideally, embeddings of text will mean that similar meaning texts have similar numerical representation.\n",
    "\n",
    "\n",
    "Our goal is to turn each of our chunks into a numerical representation (an embedding vector, where a vector is a sequence of numbers arranged in order).\n",
    "\n",
    "Once our text samples are in embedding vectors, us humans will no longer be able to understand them.\n",
    "\n",
    "However, we don't need to.\n",
    "\n",
    "The embedding vectors are for our computers to understand.\n",
    "\n",
    "We'll use our computers to find patterns in the embeddings and then we can use their text mappings to further our understanding.\n",
    "\n",
    "Enough talking, how about we import a text embedding model and see what an embedding looks like.\n",
    "\n",
    "To do so, we'll use the [`sentence-transformers`](https://www.sbert.net/docs/installation.html) library which contains many pre-trained embedding models.\n",
    "\n",
    "Specifically, we'll get the `all-mpnet-base-v2` model (you can see the model's intended use on the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#intended-uses))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53de432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I like football\n",
      "Embeddings: [-8.42475444e-02  1.00637212e-01 -3.04262508e-02 -4.04835865e-03\n",
      "  3.78771499e-02  3.74091119e-02 -9.98117849e-02  2.61343946e-03\n",
      "  1.68801006e-02 -1.06646698e-02 -9.36578587e-03 -4.31236029e-02\n",
      "  7.33321533e-03  1.65348276e-02  1.87982209e-02 -7.59599684e-03\n",
      " -2.98678111e-02 -2.66341437e-02  1.27094649e-02 -1.01594571e-02\n",
      "  8.85396637e-03  4.13204506e-02 -1.35907661e-02 -4.72090393e-02\n",
      " -1.78263839e-02  4.68517244e-02 -7.70462630e-03 -4.88640293e-02\n",
      " -3.99428159e-02  1.04912810e-01 -1.95646044e-02 -3.23913395e-02\n",
      " -5.61739877e-02 -5.53603768e-02  1.33881906e-06  1.68700833e-02\n",
      " -7.19039282e-03 -2.36500334e-02  6.86780587e-02  2.94983350e-02\n",
      "  4.66499925e-02  3.60918837e-03 -4.46939059e-02 -4.21173312e-02\n",
      " -1.59445629e-02  4.67798449e-02  3.78085896e-02  2.07526819e-03\n",
      " -2.71698646e-02  1.42831588e-02  5.78409992e-03 -4.98900842e-03\n",
      " -5.02601489e-02 -1.26381991e-02 -4.07727212e-02  1.14967311e-02\n",
      "  3.48693170e-02  2.09056195e-02  3.40205804e-02  4.15206663e-02\n",
      "  2.47722268e-02 -5.56736030e-02 -5.39382966e-03 -5.16379438e-03\n",
      " -4.75414358e-02  2.79134046e-02  7.07969442e-02 -1.93239446e-03\n",
      "  2.12249365e-02 -1.97552319e-04 -8.77789482e-02 -2.01519076e-02\n",
      " -4.30815248e-03  2.50273179e-02  1.37650277e-02 -5.19346260e-02\n",
      " -4.57388647e-02  5.02928644e-02  2.15829518e-02 -1.40141137e-02\n",
      "  4.01836149e-02  4.16770242e-02  6.32853014e-03  1.10610295e-02\n",
      " -3.42051201e-02  1.14572924e-02  2.74630766e-02  1.97202452e-02\n",
      "  1.11586526e-02  3.97156887e-02 -5.20722568e-02 -2.33238339e-02\n",
      "  2.82056183e-02  2.76915096e-02 -5.59915639e-02 -1.88611858e-02\n",
      " -3.26372758e-02  7.00356788e-04  1.49081303e-02 -4.69959565e-02\n",
      "  2.92534046e-02  5.01979254e-02  1.44609446e-02  1.07252412e-02\n",
      "  1.71034727e-02 -1.61904804e-02  1.94008071e-02 -3.67493033e-02\n",
      "  1.97742358e-02  3.02425101e-02 -2.32428480e-02 -3.38912457e-02\n",
      "  6.20148964e-02  7.67265409e-02 -6.87961280e-02  4.04021554e-02\n",
      "  1.47981467e-02  6.06796183e-02  1.84311457e-02  1.51239634e-02\n",
      "  2.10779812e-02 -2.61135586e-02  4.23340872e-02  5.12265973e-02\n",
      " -9.42042843e-03  6.99340506e-03 -7.13938698e-02  2.93005817e-02\n",
      " -2.08076686e-02 -3.55476327e-02 -5.80607206e-02 -3.05647217e-02\n",
      " -5.98830823e-03 -4.97027300e-03  5.04765771e-02 -6.29104972e-02\n",
      " -5.75689264e-02  6.68658316e-03 -2.14008475e-03 -1.81987360e-02\n",
      "  6.99582696e-03 -4.92517576e-02  1.43193789e-02 -2.51797074e-03\n",
      " -1.84805132e-02 -9.19775851e-03  3.74320429e-03 -1.29442718e-02\n",
      "  7.34594185e-03  3.90475616e-02  2.28566565e-02  1.10025723e-02\n",
      " -7.77295157e-02  6.71035564e-03  7.38958921e-03  1.09679969e-02\n",
      "  2.75377426e-02 -2.36063674e-02 -9.62760393e-03  1.65471248e-02\n",
      "  1.92160252e-02  3.00923996e-02 -1.09887589e-02  9.89481807e-04\n",
      " -3.55859734e-02 -6.70885220e-02  2.64994670e-02 -4.45276592e-03\n",
      " -2.56287884e-02 -2.01323945e-02 -1.35247540e-02 -7.76847359e-03\n",
      " -2.25988906e-02 -7.59532303e-02  7.82441944e-02  2.68173907e-02\n",
      "  8.24524462e-02 -3.75438146e-02 -2.34529320e-02 -3.12519372e-02\n",
      " -2.98749506e-02 -4.14341278e-02  5.76486252e-02  2.70741023e-02\n",
      " -2.69436231e-03  9.41387936e-03 -2.34416407e-02  8.54085684e-02\n",
      " -3.62553522e-02 -3.21777686e-02  4.57366817e-02 -1.55808125e-02\n",
      "  3.60688046e-02  1.48086147e-02  4.02978063e-03 -6.21285071e-05\n",
      "  1.62415057e-02 -2.41479315e-02  4.22139280e-02 -2.18119193e-03\n",
      " -1.14807263e-02 -1.54284341e-02  4.50379662e-02 -3.55473608e-02\n",
      "  1.90483294e-02 -6.55420572e-02 -6.85998350e-02 -5.79103967e-03\n",
      "  7.56263062e-02  1.33733107e-02  1.37130702e-02  1.52509874e-02\n",
      "  8.20295066e-02 -1.66048035e-02  2.32107565e-02  6.87709963e-03\n",
      "  1.20373359e-02 -3.14195789e-02  7.21315201e-03  3.55870277e-02\n",
      " -4.53375354e-02 -2.88249105e-02  8.19241442e-03  4.70319912e-02\n",
      " -2.83143278e-02 -1.70615446e-02  6.85482100e-02  2.70877257e-02\n",
      " -1.42026935e-02  1.92857292e-02  1.08334469e-02 -1.58385579e-02\n",
      "  1.85037423e-02  2.82974943e-04  5.83866686e-02 -1.97079107e-02\n",
      "  7.21546412e-02 -4.88322275e-03 -2.70334110e-02 -3.83967012e-02\n",
      "  7.76601536e-03  6.82347044e-02  2.78944224e-02  3.94895263e-02\n",
      "  1.63869504e-02 -9.74451378e-03  7.86119327e-02  3.70756770e-03\n",
      " -4.52783890e-02 -2.87718214e-02 -1.70555208e-02 -1.13031296e-02\n",
      "  6.51996117e-03  3.34491134e-02 -3.05923820e-02 -8.59767199e-02\n",
      " -5.59616908e-02  1.63754653e-02 -3.53234150e-02 -4.35333177e-02\n",
      "  1.18040092e-01 -3.00700609e-02 -2.91675869e-02  5.31348027e-02\n",
      "  8.05326272e-03 -4.70534042e-02  3.42307612e-03 -5.87369315e-02\n",
      "  7.74148153e-03 -2.87119988e-02 -1.03800287e-02 -3.28244902e-02\n",
      "  2.19958387e-02  1.60498004e-02  7.74394069e-03  2.10018698e-02\n",
      " -7.76350126e-02 -1.83953904e-02  3.70450015e-03  6.00363826e-03\n",
      "  9.62629262e-03  6.25347644e-02  1.62003674e-02  1.65151618e-02\n",
      "  1.36423847e-02 -2.73847999e-03 -5.40273497e-03 -2.26999763e-02\n",
      "  1.63578894e-02  1.99065395e-02  8.42060763e-05 -5.21967001e-03\n",
      "  8.64645559e-03  7.25503732e-03 -2.93584876e-02 -1.93263609e-02\n",
      " -2.67649256e-02 -2.36762147e-02 -1.57548226e-02 -6.06270619e-02\n",
      " -4.61213756e-03 -5.11957109e-02  2.52245907e-02  3.51109579e-02\n",
      " -1.74153540e-02 -4.54435088e-02 -3.07275299e-02 -4.58775321e-03\n",
      "  9.63255577e-03  4.35629524e-02 -2.95853261e-02 -2.98219491e-02\n",
      " -3.17400619e-02 -1.42361764e-02  4.54285927e-02  6.19675294e-02\n",
      " -2.35949587e-02  1.12946669e-04  4.58987281e-02 -1.00664580e-02\n",
      " -1.11280624e-02  5.16887195e-02  3.36655825e-02  3.80756147e-02\n",
      " -3.92877422e-02  1.62376673e-03 -5.14691696e-02  2.75075883e-02\n",
      "  4.87785004e-02 -5.81608899e-03  2.09721867e-02  3.64104025e-02\n",
      "  3.88538316e-02  3.02592758e-02  6.01187861e-03 -2.41591055e-02\n",
      "  2.66468595e-03  7.41400803e-03  5.48147364e-03 -2.19564904e-02\n",
      "  3.40091772e-02  9.45102982e-03 -3.10383886e-02 -7.56968707e-02\n",
      " -1.85929723e-02 -5.34317875e-03 -1.37087591e-02  3.24455947e-02\n",
      " -1.01340413e-01 -3.77967767e-02  5.66013567e-02 -8.38874727e-02\n",
      " -5.25772525e-03 -1.95408035e-02  2.80081723e-02  5.89934774e-02\n",
      "  2.29245927e-02  1.66056324e-02 -6.21854477e-02 -2.64082197e-02\n",
      " -1.08168565e-03  7.62561634e-02  2.62288488e-02  3.84933874e-02\n",
      " -1.47654666e-02  6.05497416e-03  6.29490316e-02 -2.53709145e-02\n",
      " -4.00721319e-02  2.32962370e-02 -6.68872893e-03  4.88928426e-03\n",
      " -4.29552346e-02  1.11140488e-02  9.09431931e-03  6.94774836e-02\n",
      "  1.64371822e-02  2.46265903e-02  3.53869423e-02 -3.55380140e-02\n",
      "  5.38688377e-02 -3.27346958e-02 -2.74584256e-02  2.26424094e-02\n",
      " -1.24993389e-02 -6.74723648e-03  2.70586535e-02 -2.07670201e-02\n",
      " -2.06278004e-02 -2.85870507e-02  4.28662784e-02  4.11972357e-03\n",
      " -1.10182919e-01 -6.22343831e-02  2.10633576e-02  1.69574760e-03\n",
      " -3.27287726e-02  4.77167368e-02  1.11153778e-02  2.17695013e-02\n",
      " -2.52284436e-03  5.02349585e-02 -5.10567008e-03  3.80875282e-02\n",
      " -1.71695054e-02  3.43375392e-02 -2.13631894e-02  1.73857808e-02\n",
      "  2.54417323e-02 -5.33008687e-02 -2.64375508e-02  2.22395211e-02\n",
      "  9.24143288e-03 -5.06643904e-04 -2.25211028e-03 -2.99091935e-02\n",
      "  1.48480721e-02  1.35684395e-02  6.24463521e-02 -4.34339345e-02\n",
      " -4.65225428e-02  3.55718378e-03  9.30303484e-02 -1.63256802e-04\n",
      " -1.74396727e-02  1.16810566e-02  2.71258615e-02  3.93609926e-02\n",
      " -3.20859961e-02 -3.65100205e-02  2.23816447e-02 -3.07413377e-02\n",
      " -2.08535697e-02  5.93941882e-02 -1.34406704e-02 -8.53671879e-02\n",
      " -2.68324241e-02  6.50370643e-02 -5.58489710e-02  2.90103573e-02\n",
      " -2.32818816e-02  1.80063280e-03  1.86815858e-03 -5.57178669e-02\n",
      "  3.62724550e-02 -1.13221072e-02 -4.15317565e-02 -4.26012687e-02\n",
      " -3.14608887e-02  5.70976920e-02  1.88349315e-03  3.27453464e-02\n",
      " -2.14030165e-02  7.43865222e-02  1.53319761e-02 -2.07311586e-02\n",
      "  1.98257584e-02 -1.84399970e-02  4.12098393e-02  3.26042734e-02\n",
      "  7.27778897e-02  7.42723513e-03  9.58934601e-04  2.86802519e-02\n",
      "  2.15338040e-02 -4.33621183e-02 -1.46020185e-02  8.65049008e-03\n",
      "  1.58613957e-02 -1.55544132e-02 -4.37542610e-02 -1.35810710e-02\n",
      " -2.85271667e-02 -3.34967277e-03  4.43325415e-02  7.98118785e-02\n",
      "  7.44022243e-03 -6.09604828e-02 -4.35516750e-03 -5.11232764e-02\n",
      " -7.27273407e-04  5.59347384e-02 -1.33643262e-02  2.02866886e-02\n",
      "  2.60682851e-02 -6.01524208e-03 -3.60819884e-03 -1.76506452e-02\n",
      "  2.79661622e-02 -4.57974933e-02  4.21607159e-02 -2.65814550e-02\n",
      " -1.72598585e-02 -3.59776467e-02 -2.97338795e-02  5.95300533e-02\n",
      "  1.74915176e-02 -2.48561557e-02  5.11112018e-03 -6.37427531e-03\n",
      " -8.68559033e-02  3.45525425e-03  1.45450784e-02  5.42752743e-02\n",
      "  7.10134879e-02 -3.47637646e-02  2.00921372e-02 -1.34610413e-02\n",
      " -3.85139845e-02  2.38078786e-03  2.00647824e-02 -2.93172058e-02\n",
      "  4.07206528e-02  1.70541722e-02  2.59609725e-02 -4.38436749e-04\n",
      "  2.57690102e-02 -6.63422868e-02 -5.44347614e-02 -3.35697941e-02\n",
      "  3.87411052e-03  2.16774680e-02  4.67668613e-03  3.12814564e-02\n",
      " -1.34410383e-02  4.32044789e-02  8.03099573e-03 -4.20283489e-02\n",
      "  6.53893827e-03 -2.43937876e-03 -6.02266705e-03 -3.47500034e-02\n",
      "  3.65685150e-02 -5.30309379e-02 -5.38329668e-02  7.03010336e-03\n",
      "  4.12302651e-02  8.45791921e-02 -5.03538549e-02  7.66277164e-02\n",
      " -2.78607514e-02  7.90916905e-02  3.97939198e-02 -4.55360152e-02\n",
      "  3.14156413e-02 -2.69393646e-03  5.28775081e-02  2.48770341e-02\n",
      "  4.99717146e-03  1.64268222e-02 -1.69216115e-02  2.48392168e-02\n",
      " -1.22535098e-02 -3.69962007e-02 -6.34204447e-02 -6.39738409e-33\n",
      "  6.64366176e-03  1.38151860e-02 -2.38952041e-02 -4.83649084e-03\n",
      " -4.55578007e-02 -5.83923794e-02  3.37188281e-02  1.17862355e-02\n",
      "  1.92115437e-02  1.29556293e-02 -3.08516640e-02 -1.15106320e-02\n",
      " -3.26733425e-06  5.76784555e-03  4.48282734e-02 -6.07790463e-02\n",
      " -9.49648488e-03 -7.60856504e-03  1.54688144e-02 -1.52527320e-03\n",
      " -1.90541204e-02 -7.09424901e-04  1.32276537e-02  1.74561199e-02\n",
      "  1.25597557e-02  5.03845662e-02  2.25027502e-02 -3.98198329e-02\n",
      "  2.12818366e-02  3.72618735e-02  1.24223507e-03 -1.56132123e-02\n",
      " -9.33839288e-03 -3.27454396e-02  6.25485647e-03  1.05579302e-01\n",
      " -9.70495399e-04 -4.00631800e-02 -1.75544936e-02  8.08217749e-03\n",
      "  3.00003793e-02 -3.68314274e-02  6.16536699e-02 -5.05825318e-02\n",
      " -3.28604430e-02 -5.53884991e-02  4.49809805e-02  1.67714935e-02\n",
      "  3.44181322e-02 -6.66545480e-02 -3.80341224e-02  2.98349350e-03\n",
      " -1.30312219e-02 -5.92840230e-03 -2.02847106e-04 -4.41355035e-02\n",
      "  3.82058099e-02  4.26402129e-02 -4.14671237e-03  1.25948098e-02\n",
      " -4.92177121e-02  2.28946446e-03 -3.53488661e-02 -1.31224161e-02\n",
      "  8.84506572e-03  2.31276583e-02  1.33539438e-02  4.80641099e-03\n",
      " -1.04382578e-02 -5.61291501e-02 -1.79180149e-02 -1.29853068e-02\n",
      "  8.10094364e-03 -2.99938638e-02 -3.34588066e-03  1.79704893e-02\n",
      " -6.27134964e-02 -5.14751077e-02  1.47673720e-02  4.13221419e-02\n",
      "  5.43307001e-03  1.83464121e-02 -2.86416654e-02  2.01381017e-02\n",
      "  3.06110419e-02  1.15998611e-02  1.17117418e-02 -5.30699920e-03\n",
      " -7.85787776e-02 -2.07856763e-02  1.49661861e-02 -5.28002940e-02\n",
      "  2.94363149e-03  2.98029892e-02 -9.11381794e-04  2.87766976e-04\n",
      " -2.35569701e-02 -1.39983194e-02 -1.47770243e-02  3.07937618e-02\n",
      "  5.67799201e-03  3.50137390e-02  5.42525901e-03 -3.60845141e-02\n",
      " -1.26140481e-02  1.03833005e-02 -3.88637669e-02  5.20079918e-02\n",
      " -2.38925163e-02 -2.68223230e-02  1.18294256e-02  3.52228247e-02\n",
      "  4.69845198e-02 -1.61579892e-03  2.04463992e-02  1.69065166e-02\n",
      " -7.57211354e-03  5.69040608e-03 -6.36684075e-02 -1.35807022e-02\n",
      "  6.38104230e-02  1.31863980e-02 -6.03351183e-02 -2.71138293e-03\n",
      "  2.25720331e-02 -3.93813439e-02 -3.64355296e-02  1.11656170e-02\n",
      " -9.08709243e-02  9.08006541e-03  1.39201712e-02 -2.59274221e-03\n",
      "  2.05434873e-07 -3.06701921e-02 -4.07751941e-04 -3.55365500e-02\n",
      "  3.81792523e-02 -2.87687639e-03  3.37544382e-02 -1.56595930e-02\n",
      "  6.45715967e-02  4.45960136e-03  8.05239305e-02 -3.16427759e-04\n",
      "  1.35497109e-03  5.99789843e-02  1.86059736e-02 -2.26561795e-03\n",
      " -6.04191199e-02  1.95420291e-02 -2.39958465e-02  1.83382966e-02\n",
      " -7.44236857e-02 -4.47304547e-02  4.86414321e-02  7.85381421e-02\n",
      "  5.60332835e-02 -1.60649810e-02  7.88276568e-02 -8.80419801e-04\n",
      " -8.79179463e-02  3.92033393e-03  1.87990703e-02  3.33910733e-02\n",
      " -9.93275177e-03 -4.11850289e-02 -2.51307264e-02 -5.34668155e-02\n",
      " -2.37743445e-02  2.73193587e-02  6.14445563e-03  1.16724148e-02\n",
      "  6.07858561e-02 -5.32350466e-02  1.30301118e-02 -3.11416555e-02\n",
      "  2.86225844e-02 -1.36504229e-02 -4.83773649e-02 -3.76974866e-02\n",
      "  2.96162674e-03 -6.93949237e-02  6.10292796e-03 -3.55679281e-02\n",
      "  1.12396795e-02 -3.84193212e-02 -3.19854654e-02  1.63941439e-02\n",
      "  1.11979037e-03  5.47177047e-02 -4.52604936e-03  3.19908746e-02\n",
      "  6.30161688e-02 -4.07110080e-02  5.63861020e-02  1.62612610e-02\n",
      "  3.57900746e-02  1.39383618e-02 -7.90975895e-03 -4.02934030e-02\n",
      "  8.85369201e-35  3.63460183e-02 -1.91291142e-02 -2.66956501e-02\n",
      "  4.00693296e-03  1.32429937e-03 -4.12939563e-02  4.45932858e-02\n",
      "  1.36174662e-02  8.88671875e-02 -2.11695414e-02 -2.07189694e-02]\n",
      "\n",
      "Sentence: I like basketball\n",
      "Embeddings: [-6.22272491e-02  4.80107591e-02 -1.92648813e-03  5.26621146e-03\n",
      "  3.23529094e-02  5.39589636e-02 -8.73231664e-02  1.51895452e-02\n",
      "  2.51223613e-02 -3.56969275e-02 -2.87536979e-02 -8.97584856e-03\n",
      "  2.68125534e-02 -7.00396206e-03  7.83177465e-03  5.70327369e-03\n",
      " -3.62499803e-02  1.01661244e-02  5.49255908e-02 -9.18151345e-03\n",
      " -8.18465743e-03  4.99124825e-02 -7.11822836e-03 -4.66862880e-03\n",
      "  3.36747132e-02 -1.31412707e-02  8.41982290e-03 -6.85987771e-02\n",
      " -1.78071354e-02  7.39389434e-02 -5.23901395e-02 -1.97608694e-02\n",
      " -3.52292508e-02 -7.30900764e-02  1.21738890e-06  7.61763193e-03\n",
      "  3.61950397e-02  6.99661206e-03  5.57773560e-02  9.73270927e-03\n",
      "  1.22919111e-04 -1.69265196e-02 -2.02891696e-02 -2.33368464e-02\n",
      " -2.35559735e-02  2.06807181e-02  5.86124398e-02 -2.03836169e-02\n",
      " -3.31249721e-02  3.63546493e-03  2.10842974e-02 -7.75726419e-03\n",
      " -1.25995716e-02 -1.40486490e-02 -2.14152876e-03  2.19501033e-02\n",
      "  8.51616040e-02  2.52602808e-02  5.93740344e-02  2.50930972e-02\n",
      " -5.17571298e-03 -9.87883881e-02 -5.06000072e-02  8.34873319e-03\n",
      "  3.93843185e-03  1.78955551e-02  4.64148857e-02 -5.88242570e-03\n",
      "  3.01868040e-02  7.60082202e-03 -5.01487218e-02 -6.40114490e-03\n",
      "  6.87575154e-03  2.76278295e-02  4.48354967e-02 -2.52016708e-02\n",
      " -6.77500386e-03 -1.60643682e-02  1.68799926e-02 -1.92416087e-02\n",
      "  8.38974398e-03  2.13928111e-02  1.95603501e-02 -7.58345239e-03\n",
      " -2.46595051e-02  1.36145363e-02  2.34820023e-02  1.91808529e-02\n",
      " -3.29352356e-02  5.85882328e-02  1.93208735e-02 -4.41486388e-02\n",
      "  1.96587238e-02  1.74410306e-02 -6.19699284e-02 -1.63614806e-02\n",
      " -3.05542280e-03  2.44475547e-02  1.20044909e-02 -2.92989947e-02\n",
      "  6.69585392e-02  5.73725663e-02  1.17297219e-02  1.33876745e-02\n",
      " -1.81509964e-02  6.35559438e-03  5.93428984e-02 -1.46127222e-02\n",
      " -1.23241162e-02  1.85175613e-02 -4.13895696e-02  2.30874750e-03\n",
      "  2.92062946e-02  5.49965762e-02 -4.41697687e-02  5.01870066e-02\n",
      "  7.52534205e-03  6.09690100e-02  1.21231005e-02  1.10416878e-02\n",
      " -2.66692811e-03 -3.27570140e-02  4.06444818e-02  1.93506461e-02\n",
      "  3.44337360e-03 -3.87356780e-03 -8.97875130e-02  1.59464478e-02\n",
      " -2.08916198e-02 -8.33414197e-02 -3.93878892e-02 -3.25133689e-02\n",
      "  1.18559077e-02 -2.30156276e-02  5.75271212e-02  2.06397548e-02\n",
      " -1.45557756e-02 -2.26486493e-02  2.05442384e-02 -1.94978006e-02\n",
      " -2.83785611e-02 -4.55863215e-02  1.95394922e-02 -2.94630118e-02\n",
      "  1.09752445e-02  5.29949227e-03 -9.80913918e-03 -5.53408684e-03\n",
      " -1.02213211e-02  2.33999509e-02 -4.02921848e-02  2.75291633e-02\n",
      " -4.53095064e-02 -1.95818041e-02 -1.57546345e-02 -1.67317782e-02\n",
      "  6.13340214e-02 -6.12059087e-02 -2.18579601e-02 -2.83095744e-02\n",
      "  6.57709874e-03  1.07351588e-02  3.09909638e-02 -1.67897847e-02\n",
      " -3.68133634e-02 -3.94485630e-02  4.17384505e-03  8.16651061e-03\n",
      " -2.43421793e-02 -8.17458890e-03 -6.37402060e-03 -4.87153716e-02\n",
      " -4.65784371e-02 -2.54185759e-02  6.84609562e-02  5.83732910e-02\n",
      " -1.69725772e-02 -2.73225140e-02 -1.19513441e-02 -3.68439108e-02\n",
      " -2.30530296e-02 -7.62688667e-02  8.09230283e-02  1.69105586e-02\n",
      "  3.43637471e-03  4.54416461e-02 -2.09342502e-02  6.06311150e-02\n",
      " -3.80077064e-02 -6.10282719e-02  3.46028432e-02 -1.51100568e-02\n",
      " -1.48591613e-02  1.23012243e-02  9.37595591e-03 -1.61875598e-02\n",
      "  2.61782445e-02  4.97116037e-02  4.25596163e-02  4.57473751e-03\n",
      " -8.06291308e-03  1.73171461e-02  2.01369021e-02 -3.39020304e-02\n",
      " -3.17735597e-02 -4.36105616e-02 -2.58636810e-02 -2.25477549e-03\n",
      "  5.99334426e-02 -1.12337815e-02  1.39307510e-02  3.13673355e-02\n",
      "  6.50872365e-02 -4.30587120e-02  3.68172452e-02 -1.43092657e-02\n",
      "  5.39796203e-02 -4.06583361e-02 -1.15284640e-02  2.45578904e-02\n",
      " -5.63447177e-02  2.39986256e-02  9.21114087e-02  3.17167640e-02\n",
      " -4.25084643e-02  2.08772682e-02  1.04092635e-01  6.45273074e-04\n",
      "  4.49362211e-03  1.23556023e-02  2.80290116e-02 -6.26675552e-03\n",
      " -3.41615118e-02  2.67868731e-02  2.72684656e-02  2.37094983e-02\n",
      "  5.77911511e-02 -5.08044660e-02 -3.28829736e-02 -1.33932161e-03\n",
      "  1.80153400e-02  6.54260442e-03  4.41364758e-02  3.87614407e-02\n",
      "  4.09928448e-02  1.11093083e-02  8.30428749e-02  1.32515226e-02\n",
      " -3.22272740e-02 -1.03748674e-02 -4.78492230e-02 -2.58052461e-02\n",
      "  3.06266490e-02  4.38783653e-02 -5.21335343e-04 -1.82461850e-02\n",
      "  6.41377643e-04  8.14758521e-03  2.18761582e-02 -5.02126776e-02\n",
      "  1.03572920e-01 -5.25411814e-02 -5.48026189e-02  4.31574583e-02\n",
      " -1.94098614e-02 -1.11775135e-03  4.48058508e-02 -9.72971506e-03\n",
      "  2.79043745e-02 -6.02692738e-03 -3.71642299e-02 -5.12182154e-02\n",
      "  6.65680645e-03  2.37309616e-02  6.85667433e-03  1.01490095e-02\n",
      " -1.43088130e-02  8.79475567e-03 -1.06945122e-02  1.40770124e-02\n",
      " -9.61114932e-03  4.29820530e-02 -8.14293884e-03  6.96218153e-03\n",
      "  2.43589617e-02 -7.83307757e-03 -1.76933557e-02 -6.24113576e-03\n",
      " -4.98083094e-03  4.62234206e-02 -6.02036342e-03 -3.83356884e-02\n",
      " -6.78839674e-03  6.51707873e-03 -6.68900320e-03 -3.43001187e-02\n",
      " -4.89469543e-02 -2.00599972e-02  4.07541217e-03 -6.32264018e-02\n",
      "  3.66248041e-02 -6.03811070e-02  7.25981519e-02  2.48777643e-02\n",
      "  1.63273816e-03 -4.14175242e-02 -4.26661633e-02  2.29716636e-02\n",
      "  6.19905302e-03  3.72708552e-02  4.64051450e-03 -3.86467464e-02\n",
      " -4.04161401e-02 -5.06673120e-02  5.01261093e-02  6.54773116e-02\n",
      "  6.42693136e-03 -2.26248223e-02  8.12507048e-03 -3.47618386e-02\n",
      " -4.99704182e-02  4.35636863e-02  8.05452373e-03  3.59530151e-02\n",
      " -4.19094972e-02  2.12677568e-03 -4.48889919e-02  3.61827086e-03\n",
      "  2.67603137e-02 -2.97530030e-04  1.75167900e-02 -1.40003059e-02\n",
      "  3.72778401e-02  3.30388211e-02  2.47552153e-02 -3.95622440e-02\n",
      "  2.53535286e-02  7.29821669e-03  8.92106153e-04 -2.82260850e-02\n",
      "  6.83249533e-02 -1.42585803e-02 -1.57718807e-02  5.28142694e-03\n",
      " -6.27221093e-02 -2.33333334e-02 -2.28384323e-02 -1.42709725e-02\n",
      " -9.02765989e-02 -8.73969495e-02  4.69247922e-02 -9.23447236e-02\n",
      "  2.17867531e-02 -3.68620008e-02  5.16158622e-03  4.51893359e-02\n",
      "  1.00311218e-02  7.26890052e-03 -7.89989084e-02 -1.37210055e-03\n",
      " -1.70900545e-03  5.36523759e-02 -3.53705212e-02  3.94686908e-02\n",
      "  2.41867956e-02  3.38796936e-02  6.61817193e-02 -5.27016399e-03\n",
      " -7.55617395e-02  5.97741045e-02  1.66395665e-04 -1.04198488e-03\n",
      " -5.59063107e-02  2.27977764e-02  9.59182624e-03  1.88448112e-02\n",
      "  4.06849608e-02  3.71507257e-02 -1.25253201e-02 -5.28873876e-02\n",
      "  3.23249772e-02 -4.81804088e-02 -3.59884091e-02  2.72231102e-02\n",
      " -3.74460556e-02 -4.26430330e-02 -8.91751982e-03 -4.94932197e-02\n",
      " -8.20840672e-02 -2.19074544e-02  4.60920706e-02  9.68975574e-03\n",
      " -6.64374828e-02 -4.73582745e-02  6.15322143e-02 -3.13153490e-02\n",
      " -9.63102281e-03  4.44769626e-03  2.47762669e-02  2.37569716e-02\n",
      " -1.19248768e-02  4.13005576e-02  5.93949622e-03  2.17993371e-02\n",
      " -2.55940817e-02 -7.14307232e-03  7.10192020e-04  2.11563129e-02\n",
      "  5.87623974e-04  1.29017094e-02 -5.34296110e-02 -1.08021847e-03\n",
      " -2.90097063e-03 -8.58804770e-03  4.89120418e-03 -1.04394108e-02\n",
      "  2.42561232e-02 -1.23261604e-02  7.60536492e-02 -3.24492678e-02\n",
      " -3.75105962e-02 -3.12218517e-02  5.68580404e-02  4.34360728e-02\n",
      " -2.96102967e-02  4.36863899e-02 -9.16757528e-03 -1.29666189e-02\n",
      " -2.72127446e-02 -5.56946099e-02  4.00549807e-02 -4.41518845e-03\n",
      " -3.05867121e-02  6.90794662e-02 -7.53296092e-02 -8.56188014e-02\n",
      " -4.88807261e-02  8.67507160e-02 -1.86916739e-02 -8.47973488e-03\n",
      " -4.14148308e-02  1.25162853e-02  2.73346305e-02 -5.32191917e-02\n",
      "  3.03267166e-02 -2.06976067e-02 -4.25292887e-02 -5.93749546e-02\n",
      " -6.63804188e-02  2.72896737e-02  6.10590633e-03  3.50520052e-02\n",
      "  8.19267333e-03  3.24757509e-02  4.93208412e-04 -1.55680897e-02\n",
      "  3.05343214e-02 -8.06736294e-03  6.51803426e-03  2.09508222e-02\n",
      "  6.14110194e-02  6.02944056e-03  1.71777122e-02  3.52091305e-02\n",
      "  3.82577702e-02 -7.22996658e-03 -2.07848218e-03  1.29131088e-02\n",
      "  4.08537798e-02 -4.66254316e-02 -3.99282239e-02 -7.12412829e-03\n",
      " -3.13827284e-02 -2.16344092e-03  2.99163163e-02  7.27892816e-02\n",
      "  2.33910456e-02 -5.00748903e-02 -3.86200100e-02 -5.37739322e-02\n",
      " -3.04967016e-02  4.18373570e-02  1.84124224e-02 -2.45365384e-03\n",
      "  6.03914307e-03 -2.18064375e-02  5.02362964e-04  1.20579433e-02\n",
      " -1.53616099e-02 -7.65968114e-02  1.73336975e-02 -4.66602519e-02\n",
      " -4.52180915e-02  1.01748090e-02  8.94448254e-03  4.97487262e-02\n",
      "  1.32857971e-02 -3.64009105e-02 -1.74232963e-02  4.98526096e-02\n",
      " -5.34444824e-02  1.85468160e-02 -6.66272221e-03  3.13199274e-02\n",
      "  5.71382940e-02 -5.18840067e-02  2.89794207e-02  2.29748711e-03\n",
      " -2.11478900e-02  2.77392231e-02  4.16600406e-02 -1.72553547e-02\n",
      " -2.20398642e-02  3.47720981e-02  2.04981696e-02  1.11271059e-02\n",
      "  1.53777916e-02 -7.84212500e-02 -4.76050489e-02 -1.80646982e-02\n",
      " -2.64949817e-03  9.62796621e-03  3.99309807e-02  2.76857372e-02\n",
      " -5.62517531e-02  1.58320963e-02  8.57149623e-03 -6.61209598e-02\n",
      " -1.29279029e-02 -2.75374912e-02 -2.73725251e-04 -1.82666220e-02\n",
      "  1.19144162e-02 -1.70310792e-02 -3.33804041e-02  7.49145299e-02\n",
      "  1.17980363e-02  7.80725554e-02 -3.94509882e-02  4.34103906e-02\n",
      " -6.44988045e-02  5.15319705e-02  1.50570972e-03 -1.96008012e-02\n",
      "  3.67573798e-02  2.49757878e-02  2.43373588e-02  7.76672037e-03\n",
      "  8.23159330e-03 -2.38803905e-02 -2.43983753e-02  1.01570543e-02\n",
      " -6.49578171e-03 -2.26200242e-02 -6.43210188e-02 -5.85809373e-33\n",
      "  5.72300516e-03 -3.05973683e-02 -3.97941284e-02 -2.75348444e-02\n",
      " -3.82245257e-02 -5.70884794e-02  3.90184112e-02  2.50704996e-02\n",
      "  5.63412532e-03  1.47009920e-02  1.54906986e-02 -2.82354429e-02\n",
      " -3.30793625e-03 -1.65408570e-02  5.01762852e-02 -3.14617939e-02\n",
      "  4.43916917e-02 -1.14849471e-02  9.43833496e-03 -1.47571256e-02\n",
      " -1.40557261e-02  4.11555462e-04  3.25406231e-02  3.85502428e-02\n",
      "  3.99366692e-02  4.05447334e-02  6.32582754e-02 -1.42610576e-02\n",
      "  4.42889370e-02  3.41823958e-02  2.03564836e-04 -3.84817496e-02\n",
      " -2.23850142e-02 -1.92972613e-04  1.08099012e-02  6.29928857e-02\n",
      " -4.04752605e-02 -5.48928268e-02 -2.58665979e-02  3.75302061e-02\n",
      " -5.99690014e-03 -5.90909906e-02  4.43671383e-02 -4.23974618e-02\n",
      " -3.08833700e-02 -6.95434511e-02  4.37683277e-02  7.59853888e-03\n",
      "  2.81342026e-02  2.10340694e-02 -5.87086752e-02  6.04338653e-04\n",
      " -1.15476539e-02  1.26211336e-02  3.59117426e-03 -3.30250710e-02\n",
      "  4.99690771e-02  5.28940633e-02  2.40658373e-02  5.42520173e-03\n",
      " -1.01914175e-01  1.02941936e-03 -6.09651208e-02 -1.93752535e-02\n",
      "  1.88033786e-02  1.38115659e-02  5.69404513e-02 -2.24382058e-02\n",
      "  6.04909053e-03 -5.67644052e-02  3.10505228e-03  1.35167344e-02\n",
      "  9.48604569e-03  9.58835764e-04 -1.49123780e-02  5.22852503e-02\n",
      " -3.10254414e-02 -4.53669317e-02  1.41081056e-02  1.99068021e-02\n",
      " -2.84266677e-02  3.74705233e-02 -2.82629710e-02 -3.09950439e-03\n",
      "  6.50089756e-02  4.12673168e-02 -7.24415854e-03 -1.28572807e-02\n",
      " -6.20873123e-02 -2.51292214e-02  2.26080250e-02 -7.60199642e-03\n",
      " -2.05126163e-02  3.61304879e-02 -3.47006433e-02  5.85581511e-02\n",
      " -2.35079415e-02 -2.36007385e-02 -2.93973349e-02  2.77464483e-02\n",
      " -3.03294454e-02  3.50275598e-02  1.95699073e-02 -9.48888133e-04\n",
      " -2.28838138e-02 -2.76688300e-03 -2.84795873e-02  2.48472914e-02\n",
      "  3.78585490e-03  8.71540885e-03 -1.25125647e-02  2.02094764e-02\n",
      "  6.69893250e-02  3.83571759e-02  4.20986116e-02  3.21502946e-02\n",
      "  5.06816339e-03  2.82208044e-02 -4.37153764e-02 -5.24876900e-02\n",
      "  2.33595334e-02  4.01501320e-02 -5.28206378e-02 -1.88823212e-02\n",
      "  2.65419278e-02 -3.76599729e-02 -1.30796004e-02  3.60551365e-02\n",
      " -8.52108672e-02  1.03724552e-02 -5.09973103e-03 -2.75096409e-02\n",
      "  1.98919423e-07 -4.80021760e-02 -2.81328079e-03 -5.49867079e-02\n",
      "  5.07247783e-02 -5.01032099e-02  3.99160907e-02  1.22621255e-02\n",
      "  5.58770373e-02  1.81159265e-02  7.31264800e-02  3.04164700e-02\n",
      " -1.85559541e-02  6.41402453e-02 -3.08568124e-03 -9.02264751e-03\n",
      " -2.17575543e-02  7.57232634e-03  1.21865105e-02  8.79124645e-03\n",
      " -4.37642224e-02 -2.09047329e-02  8.48707929e-03  6.55811876e-02\n",
      "  5.24271801e-02  2.16284464e-03  9.67016723e-03  4.50440198e-02\n",
      " -6.29814342e-02  1.70640498e-02  3.81360762e-02  3.27997021e-02\n",
      "  1.73394755e-02 -3.52188908e-02 -2.37623602e-02 -6.45277500e-02\n",
      " -2.38861050e-02  2.92503722e-02 -1.90313943e-02 -9.24785528e-03\n",
      "  9.03677046e-02 -2.22485978e-02  7.60711059e-02 -3.77963670e-02\n",
      "  2.71167047e-02 -4.67768405e-03 -2.44298037e-02 -3.81839052e-02\n",
      "  9.57340654e-03 -4.68689948e-02  1.88797805e-02 -7.61640146e-02\n",
      "  3.02860904e-02 -3.40487435e-02  2.00572014e-02  6.23896718e-03\n",
      "  1.93612110e-02  5.93664087e-02  1.79113622e-03  4.11414281e-02\n",
      " -2.39268448e-02 -3.90329584e-02  1.00109674e-01  2.32497952e-03\n",
      "  4.22791950e-02  3.00961547e-02  3.52622494e-02 -1.95103362e-02\n",
      "  8.62984414e-35 -1.43457614e-02 -2.40049846e-02 -5.96125936e-03\n",
      "  1.58402491e-02 -4.83821593e-02 -2.64609400e-02  3.89628783e-02\n",
      "  1.60151012e-02  5.20297848e-02  1.09243754e-03 -4.58407439e-02]\n",
      "\n",
      "Sentence: Kobe is a great player\n",
      "Embeddings: [-2.82118525e-02 -9.04116780e-03  1.47323152e-02 -9.61336866e-03\n",
      "  6.27707643e-03 -9.59475152e-03 -1.23444218e-02 -1.73035599e-02\n",
      " -2.20347997e-02  2.16303603e-03 -3.87667231e-02  3.85243781e-02\n",
      "  2.45300960e-02 -4.04894985e-02 -3.95844830e-03  9.28172749e-03\n",
      "  3.49956192e-02  4.87582497e-02  1.20649815e-01  2.80728042e-02\n",
      "  2.89687999e-02 -1.19733755e-02  3.87907512e-02  6.53742161e-03\n",
      " -1.24635538e-02 -2.27510203e-02  3.98658402e-02 -6.08220808e-02\n",
      " -1.95839498e-02  1.09908544e-02 -1.03089973e-01 -2.95806639e-02\n",
      " -6.17376305e-02 -5.18394448e-02  1.39980489e-06  6.38066325e-04\n",
      "  6.61658347e-02 -7.09913496e-04  1.86926015e-02  7.20426627e-03\n",
      " -3.76049280e-02 -1.00609511e-02  7.16739986e-03 -5.19050509e-02\n",
      "  4.00629733e-03  4.50971946e-02  3.91169637e-02  6.94102943e-02\n",
      "  2.56250296e-02 -1.99230183e-02  4.82478105e-02 -4.29571085e-02\n",
      "  1.74177699e-02 -3.12592611e-02 -3.52522880e-02  4.95111495e-02\n",
      "  6.21278062e-02  2.52808910e-02 -1.31634632e-02 -1.07298335e-02\n",
      " -2.16147285e-02 -2.07273830e-02 -5.46170846e-02 -2.77616605e-02\n",
      "  1.09067574e-01 -3.34125645e-02  4.00532484e-02  3.29329781e-02\n",
      "  6.67546242e-02  1.51396133e-02 -3.45993973e-02  2.58169007e-02\n",
      "  5.16696926e-03 -7.58178439e-03  3.75934839e-02 -6.08188473e-02\n",
      " -1.54259067e-03 -1.02525670e-02  7.95563124e-03 -1.65775921e-02\n",
      " -5.00032119e-02 -2.80231726e-03  2.29841471e-02 -3.46449390e-02\n",
      " -7.64422119e-03 -2.04899944e-02  2.09594779e-02 -1.11607567e-03\n",
      " -5.63489795e-02 -5.06420713e-03 -3.32728699e-02 -3.53483371e-02\n",
      " -8.00071575e-04  3.41142970e-03  1.47614861e-02  1.11595849e-02\n",
      "  1.52857946e-02  6.83271065e-02 -2.96210833e-02 -8.22365358e-02\n",
      "  4.39710841e-02  1.69113986e-02  4.83271070e-02 -9.88003053e-03\n",
      " -5.61211538e-03  7.20386356e-02 -7.80361937e-03 -2.69338265e-02\n",
      "  1.48752509e-02  1.96265038e-02 -1.62536465e-02  1.33487368e-02\n",
      "  3.58128385e-03  1.26843108e-02 -1.46175195e-02  3.03835869e-02\n",
      " -1.95606072e-02  3.26657994e-03 -3.08693275e-02 -1.12716267e-02\n",
      "  7.42430193e-03 -6.32353574e-02 -1.34774437e-02  1.71646327e-02\n",
      " -4.03368175e-02  7.90505763e-03 -9.64638144e-02  1.63418576e-02\n",
      " -6.33923262e-02 -1.79759581e-02 -3.19660008e-02 -1.47585357e-02\n",
      "  9.87625774e-03  3.99344414e-02  4.94580306e-02 -4.64827120e-02\n",
      "  1.48687232e-02 -4.51283231e-02  2.72384249e-02  4.32527922e-02\n",
      " -4.22702916e-02 -1.53702851e-02  1.19535374e-02 -2.87613980e-02\n",
      "  3.20154242e-02  2.27719657e-02  3.00138909e-02 -3.54787195e-03\n",
      " -5.46104182e-03  6.51898012e-02  3.92587809e-03 -3.91748026e-02\n",
      "  5.60619449e-03 -3.01766135e-02  1.56902578e-02 -2.87288185e-02\n",
      "  3.96909714e-02 -5.35050519e-02  2.47440636e-02 -7.01344013e-02\n",
      " -2.63235141e-02 -2.77276076e-02  6.12143101e-03  1.15330685e-02\n",
      " -6.91011325e-02 -1.17060225e-02  9.72461421e-03  1.41950126e-03\n",
      " -4.87588756e-02  4.43486832e-02 -3.78297493e-02  6.91233529e-03\n",
      " -8.33775923e-02  4.74074408e-02  2.28677243e-02 -3.94263342e-02\n",
      " -6.15095086e-02  4.55096290e-02 -3.30302306e-03  2.56435908e-02\n",
      "  8.00985470e-03 -4.79364693e-02 -2.73989365e-02  3.70797142e-02\n",
      "  9.93705392e-02  6.37206342e-03 -1.06701523e-01  5.32008521e-02\n",
      "  3.71813811e-02 -5.29544279e-02 -1.70130078e-02  2.94457506e-02\n",
      " -3.90315577e-02 -3.56740206e-02  2.04433147e-02  1.75124090e-02\n",
      "  4.16667722e-02  1.08828716e-01 -4.95580630e-03  8.94688070e-03\n",
      " -2.46939948e-03 -1.05523588e-02  2.07498707e-02 -8.01233649e-02\n",
      "  1.71008855e-02  4.01311256e-02  4.60258834e-02 -1.14087006e-02\n",
      "  4.00634259e-02 -5.61336800e-03 -4.11555693e-02  7.71431299e-03\n",
      "  5.06581925e-03 -4.12896983e-02  3.78354304e-02  1.54614006e-03\n",
      "  5.99360988e-02  2.62340009e-02  1.23164458e-02  3.02056801e-02\n",
      " -1.08005842e-02 -5.91418631e-02  8.70986730e-02  1.38603514e-02\n",
      " -1.33855473e-02 -5.45361787e-02  3.43634747e-02 -6.27224222e-02\n",
      "  2.84420941e-02  2.98906434e-02 -4.49189451e-03  4.98954169e-02\n",
      " -2.95415544e-03  1.65495798e-02 -1.56008527e-02  6.91428632e-02\n",
      " -3.61566097e-02 -1.58006779e-03 -3.89778428e-02 -2.73550469e-02\n",
      "  4.78531327e-03 -1.26937227e-02  3.80774960e-02 -2.90945396e-02\n",
      " -1.94594171e-02  1.70382753e-03  1.00991301e-01  2.95712985e-02\n",
      "  2.35348959e-02 -4.27234247e-02 -1.85795259e-02 -4.26407205e-03\n",
      " -3.61659867e-03  2.95286458e-02  1.83480419e-02 -3.23805735e-02\n",
      "  7.29176924e-02  6.82157278e-02  6.90440089e-02 -2.12516617e-02\n",
      "  4.05838108e-03  7.54929613e-04 -2.10751174e-03  1.25279594e-02\n",
      "  6.54754490e-02 -1.36684850e-02  6.78714737e-03 -3.62858437e-02\n",
      "  3.79247479e-02  3.14189382e-02  4.60656174e-03 -2.43665110e-02\n",
      "  3.89632620e-02  2.24890783e-02 -2.40311045e-02 -9.78498347e-03\n",
      " -2.86865048e-02  3.68277617e-02 -3.82541530e-02 -5.26750460e-03\n",
      "  5.13897371e-03 -1.19900536e-02 -1.80793144e-02 -1.24877589e-02\n",
      "  3.73000801e-02  4.42280574e-03 -2.99135339e-03  2.56087128e-02\n",
      " -3.43990773e-02 -2.75580902e-02 -1.23009980e-02 -4.25507203e-02\n",
      "  3.73038538e-02  1.06519489e-02  1.48978932e-02  9.28254146e-03\n",
      " -3.52154076e-02  2.17907410e-02 -3.42823565e-03 -9.03730839e-03\n",
      "  5.39099015e-02 -8.29852745e-02  6.56244457e-02  8.94746277e-03\n",
      "  5.00975326e-02 -9.76906810e-03 -2.28925515e-02  8.36866498e-02\n",
      "  3.62874642e-02  5.69830928e-03  1.92582812e-02  1.28917573e-02\n",
      " -3.86314280e-03 -6.11638501e-02  2.44999137e-02 -3.23809907e-02\n",
      "  1.63926017e-02 -9.64769255e-03 -3.52175348e-02 -1.06267415e-01\n",
      " -1.56528968e-02  2.80204713e-02 -1.32153230e-02 -4.92548710e-03\n",
      "  2.49151178e-02  9.04063229e-03  9.04199481e-03  2.01389510e-02\n",
      " -2.26178877e-02  3.50510590e-02  1.52031761e-02 -2.52178777e-03\n",
      "  9.92886163e-03  2.95503978e-02  4.89426702e-02  1.53732467e-02\n",
      "  5.21792695e-02 -9.58876461e-02 -9.95557010e-03 -3.37995030e-02\n",
      "  3.08098812e-02 -9.12985113e-03 -7.63334474e-03  7.64195994e-02\n",
      " -7.72487894e-02 -3.74733508e-02  1.38439322e-02 -7.56263360e-03\n",
      " -9.73477364e-02 -3.08663361e-02  6.25444623e-03 -1.05692968e-01\n",
      "  7.25860102e-03 -5.22184744e-02  4.41941470e-02  2.27540806e-02\n",
      " -7.09983893e-03 -2.43348777e-02 -3.79495211e-02 -8.58548377e-03\n",
      " -2.73742508e-02  6.04124181e-02 -3.96131985e-02  3.12218890e-02\n",
      "  1.25986291e-02 -3.82516580e-03  1.01403436e-02  1.27013018e-02\n",
      " -1.97142009e-02  1.89646147e-02  6.22661747e-02 -2.38338355e-02\n",
      " -8.36420432e-02  4.03325297e-02 -3.96144198e-04  2.23353859e-02\n",
      " -2.01560557e-03 -5.02762618e-03 -3.59465964e-02 -3.94813977e-02\n",
      " -2.51413006e-02  1.31257251e-02 -6.94100857e-02  8.81774258e-03\n",
      " -7.61652645e-03 -1.36984792e-02  3.94006819e-02  1.84967853e-02\n",
      " -1.53238093e-02  4.31566760e-02  3.45505960e-02  1.75086304e-03\n",
      " -2.99360827e-02 -6.11203499e-02  5.21622002e-02 -1.88613851e-02\n",
      "  1.09453946e-02 -8.11145362e-03  1.76527502e-03 -3.76036726e-02\n",
      " -4.08613868e-02  4.67633121e-02 -5.39064361e-03  5.03118224e-02\n",
      " -6.63680211e-03 -3.09437048e-02  3.13354656e-02  7.75672272e-02\n",
      "  3.44914058e-03  1.49875488e-02 -4.24690843e-02  4.92445678e-02\n",
      "  1.93172190e-02 -2.29831989e-04 -3.89764011e-02 -2.44477205e-02\n",
      "  1.11007839e-02 -7.65819326e-02  1.51059944e-02  2.37614308e-02\n",
      " -3.46031897e-02 -2.33574919e-02  2.51610354e-02  1.60388388e-02\n",
      " -1.90702248e-02  7.37922871e-03 -1.59644615e-02 -7.31789833e-03\n",
      " -2.46850178e-02 -1.45876557e-02  2.87357830e-02 -1.73507910e-02\n",
      "  2.34455359e-03 -1.31595992e-02  3.12542915e-02 -3.82079519e-02\n",
      "  2.27644667e-03 -3.83421080e-04  1.74544323e-02 -3.13523635e-02\n",
      " -3.45121138e-02 -2.45939847e-02 -3.00451070e-02  8.89339671e-03\n",
      " -1.82946362e-02 -2.78982930e-02 -8.96145776e-03  1.14380512e-02\n",
      "  4.77330963e-04  2.53427271e-02 -4.17809337e-02  1.10710692e-02\n",
      "  2.56903637e-02  6.13985658e-02 -1.15502430e-02  6.36300072e-04\n",
      " -6.48147333e-03  1.15178600e-02  2.95234546e-02  2.09400505e-02\n",
      "  7.11889472e-03  4.78506275e-03 -2.31199292e-03  2.62122769e-02\n",
      "  2.70864996e-03  2.30019484e-02  6.66228607e-02  2.11886484e-02\n",
      " -2.70784292e-02 -1.65114067e-02 -6.92118704e-02  1.53120989e-02\n",
      "  2.85270857e-03  1.39746005e-02  1.71007812e-02 -5.82010336e-02\n",
      " -3.11968513e-02 -7.39365667e-02 -3.21909301e-02 -4.75111790e-02\n",
      "  3.00381090e-02  3.97835635e-02  2.30940878e-02  4.62270528e-03\n",
      " -1.05134901e-02  2.00438057e-03 -2.46795956e-02  3.19098122e-02\n",
      "  1.33889518e-03 -4.39205300e-03  1.65139567e-02  6.40924647e-02\n",
      " -5.82538061e-02  6.05147071e-02 -4.27788720e-02 -2.02401169e-02\n",
      " -1.04733249e-02  2.28739064e-02  4.09436822e-02 -6.77453279e-02\n",
      " -5.72763430e-03 -2.64745932e-02 -1.62202679e-02 -5.19788936e-02\n",
      "  5.22521697e-03 -6.45656511e-02  2.64531914e-02  1.57226156e-02\n",
      " -2.42822468e-02  7.40127871e-03  3.46643478e-02  4.14328165e-02\n",
      " -4.67906930e-02  6.23079529e-03  1.13450028e-02  3.67483348e-02\n",
      "  3.26659977e-02 -7.60764210e-03 -2.63546519e-02  2.62175519e-02\n",
      " -1.88959613e-02  3.50738913e-02  5.12670577e-02 -2.13443339e-02\n",
      " -5.60437925e-02 -4.98619489e-02  2.80338340e-02 -3.74242403e-02\n",
      " -2.94337291e-02 -7.22687989e-02 -5.77903986e-02 -3.44954319e-02\n",
      " -7.98855498e-02  7.61008851e-05  1.07952894e-03  4.77356911e-02\n",
      "  1.07565578e-02  1.33897141e-02 -9.36673488e-03 -6.32448262e-03\n",
      " -3.25812511e-02  7.75129348e-03  2.00480595e-02 -4.41260636e-03\n",
      "  3.40627879e-02  5.22447564e-02 -1.00611029e-02  3.15408148e-02\n",
      "  1.62822902e-02 -4.52730097e-02 -1.84213761e-02 -6.38789823e-03\n",
      " -5.94686456e-02  6.42821472e-03  2.95223948e-02 -4.51839421e-33\n",
      "  2.26026662e-02  6.71956083e-03  3.37767205e-03 -1.45988939e-02\n",
      " -4.23202887e-02 -7.68176317e-02  9.29052592e-04  7.94126187e-03\n",
      " -1.65877659e-02  4.04102281e-02 -1.14784846e-02  5.48529774e-02\n",
      "  1.66666638e-02  1.22464914e-02  4.35367823e-02 -4.47990149e-02\n",
      "  3.11674122e-02 -6.85791904e-03 -1.47530362e-02  2.86404695e-02\n",
      " -1.49253923e-02 -1.81328338e-02 -2.42196769e-02 -3.69186234e-03\n",
      "  7.37573430e-02 -1.49182826e-02 -8.79462156e-03  2.17344146e-02\n",
      "  2.43539102e-02  2.03567781e-02 -8.33725650e-03  6.66982308e-03\n",
      " -5.00087347e-03  3.54147777e-02 -1.43355783e-02  6.73355758e-02\n",
      " -3.37242484e-02  1.08639598e-02 -4.11371104e-02  3.27677019e-02\n",
      " -1.65638868e-02  2.61335149e-02  1.12389931e-02 -2.24228892e-02\n",
      "  3.83957895e-03  4.06702757e-02 -1.71461590e-02 -3.22076096e-03\n",
      "  3.89411650e-03 -2.19590962e-02 -3.80509682e-02 -2.89139198e-03\n",
      " -2.14842986e-02  4.38692942e-02 -5.94137274e-02 -4.37597781e-02\n",
      " -1.07475212e-02  3.90630439e-02 -7.50316030e-05  4.13619308e-03\n",
      " -3.51980552e-02 -5.86198680e-02 -2.78871264e-02  4.57842276e-03\n",
      "  6.45194873e-02  8.75054020e-03  4.62152883e-02 -2.66618282e-02\n",
      "  4.59132977e-02 -5.38304523e-02 -2.25949679e-02  3.62461135e-02\n",
      "  3.68288788e-03  9.47648734e-02 -9.12569184e-03  2.52024680e-02\n",
      "  4.17721234e-02 -2.96226088e-02 -5.21049788e-03  3.02704703e-02\n",
      " -2.70659309e-02  2.51187775e-02 -6.25919551e-02  8.09839834e-03\n",
      "  2.79025212e-02 -2.19831802e-02  1.08336164e-02  1.51417851e-02\n",
      "  2.06056926e-02 -5.37605621e-02 -6.22554310e-03 -4.66736332e-02\n",
      "  1.38660902e-02  8.99282005e-03  9.43289232e-03  2.00360399e-02\n",
      " -3.32086459e-02 -4.92762215e-02 -2.08499376e-02 -1.68992335e-03\n",
      " -2.00822223e-02  2.65767053e-02  1.86196025e-02 -1.90073941e-02\n",
      " -2.28595617e-03  1.65688246e-02 -2.86864061e-02 -1.47141684e-02\n",
      " -4.36687982e-03 -8.86137970e-03 -4.03925665e-02 -3.34767029e-02\n",
      " -3.54132731e-03  5.10584610e-03 -1.61204133e-02  1.39268450e-02\n",
      "  1.11534148e-02  1.49048911e-02 -2.62586474e-02  2.55388841e-02\n",
      " -1.07759684e-02  1.32134538e-02 -5.14362976e-02  1.86543178e-03\n",
      "  1.47883408e-02 -8.26745946e-03  5.26817841e-03  2.96338946e-02\n",
      " -3.48620899e-02 -2.37120446e-02 -3.87853459e-02 -7.96373412e-02\n",
      "  2.11507157e-07  1.07965916e-02  3.41437608e-02 -5.92588484e-02\n",
      " -8.03548563e-03 -3.82074974e-02  1.11587957e-01 -2.19877753e-02\n",
      "  1.21182222e-02  4.94650332e-03  9.92240384e-02  6.70098662e-02\n",
      " -3.26306745e-02  4.85678762e-03  9.97680891e-03 -1.38606904e-02\n",
      " -1.59668140e-02  4.55764420e-02 -9.50156376e-02  2.01130491e-02\n",
      " -5.36803044e-02  1.73400547e-02 -4.00989689e-02  8.62210337e-03\n",
      "  1.32863978e-02 -2.67898515e-02 -7.72122815e-02  6.46701530e-02\n",
      " -1.82850584e-02  2.01686285e-02  1.79848145e-03  7.39833117e-02\n",
      " -4.67263795e-02  2.34149620e-02  2.79222131e-02 -5.17243035e-02\n",
      " -2.94224899e-02 -3.91042866e-02  5.51042743e-02  8.90349736e-04\n",
      "  5.51502034e-02  1.83935389e-02 -1.31269088e-02 -2.36003771e-02\n",
      "  4.59791571e-02 -1.05446130e-02  2.21048556e-02 -5.44401482e-02\n",
      " -5.71312606e-02 -6.92302957e-02  3.45594175e-02  3.31442128e-03\n",
      "  2.77298000e-02 -6.72984496e-03  5.24469055e-02  8.64215568e-03\n",
      " -2.20127609e-02  2.70857774e-02  2.48317178e-02  7.83131737e-03\n",
      " -7.51938224e-02 -1.04140248e-02  7.12444782e-02  7.11087286e-02\n",
      "  7.08477497e-02  1.40701272e-02  6.24108538e-02  3.48708108e-02\n",
      "  7.80911375e-35 -1.22509981e-02 -2.13047862e-02  5.00691980e-02\n",
      "  1.84837710e-02 -4.78601642e-02 -4.66456520e-04 -1.69643741e-02\n",
      "  1.81279313e-02  3.87187935e-02  7.45013505e-02 -1.97942164e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model=SentenceTransformer(model_name_or_path='all-mpnet-base-v2', device='cpu')\n",
    "\n",
    "#Create a list of Sentences\n",
    "sentences=[\"I like football\",\"I like basketball\", \"Kobe is a great player\"]\n",
    "\n",
    "#Sentences are encoded/embedded by calling model.encode()\n",
    "embeddings=embedding_model.encode(sentences)\n",
    "embeddings_dict=dict(zip(sentences, embeddings))\n",
    "\n",
    "#See embeddings\n",
    "for sentence, embeddings in embeddings_dict.items():\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Embeddings: {embeddings}\")\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb9e69",
   "metadata": {},
   "source": [
    "Woah! That's a lot of numbers.\n",
    "\n",
    "How about we do just once sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80e9dd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Who is Michael Jordan?\n",
      "Embedding:\n",
      "[ 1.48610771e-03 -1.29408650e-02  4.12214510e-02  1.48524977e-02\n",
      "  4.33810474e-03  1.77816706e-04  1.77243222e-02 -1.45564685e-02\n",
      "  3.38754840e-02 -3.48900668e-02  2.58417353e-02  2.15714537e-02\n",
      "  1.22249816e-02  2.01746691e-02  3.11459322e-02 -1.97694656e-02\n",
      "  2.88256221e-02 -2.19796542e-02  1.69615056e-02 -6.30995631e-03\n",
      " -3.76892760e-02  3.13616637e-03  1.31576005e-02  4.23933007e-02\n",
      " -6.79306760e-02 -3.42432857e-02  1.42761124e-02 -1.04357312e-02\n",
      " -3.40646617e-02  2.10747235e-02 -5.73627837e-02 -3.49448919e-02\n",
      "  3.54876113e-03 -5.57608232e-02  1.40041789e-06  8.85181408e-03\n",
      "  5.87321296e-02 -1.26579041e-02  4.14434522e-02 -3.53717878e-02\n",
      " -3.66200916e-02 -4.22541238e-02 -6.18494349e-03  2.17272546e-02\n",
      " -1.62388813e-02 -3.43839340e-02  1.98135171e-02 -2.18353979e-02\n",
      " -1.34257283e-02  9.53534525e-03  4.17782478e-02 -1.07229194e-02\n",
      "  3.07925977e-02 -3.29672098e-02 -3.48641463e-02 -1.76475141e-02\n",
      "  2.64758412e-02  5.72366677e-02 -9.53142047e-02 -3.24030854e-02\n",
      " -1.19644981e-02  2.52823140e-02 -6.15231581e-02 -2.27554180e-02\n",
      "  4.81360517e-02 -7.98497535e-03 -1.69439353e-02  2.49707457e-02\n",
      "  2.03291439e-02  1.36252111e-02 -4.10298333e-02 -2.11949591e-02\n",
      "  4.10123952e-02  9.01053101e-02  2.44443491e-02  4.35017273e-02\n",
      "  4.66371700e-02  6.62337914e-02  4.01614159e-02 -3.96601856e-02\n",
      " -2.29190756e-02  3.24564204e-02  2.53425986e-02 -1.19209150e-02\n",
      " -2.04656832e-02  1.90375056e-02 -6.77721715e-03  2.04240903e-02\n",
      " -9.92860645e-03  1.03868628e-02  7.98518807e-02 -5.79214059e-02\n",
      "  4.27643545e-02  6.99010715e-02 -2.16403287e-02 -3.93934846e-02\n",
      " -3.51162348e-03 -9.64881200e-03 -6.95140660e-02 -6.51811361e-02\n",
      "  7.20926421e-03  4.76903617e-02  1.18859455e-01 -7.05361075e-04\n",
      "  2.89608035e-02 -1.77572668e-03 -9.18025710e-03 -5.14665544e-02\n",
      " -3.56493071e-02 -3.49588543e-02 -6.38287514e-02  1.71572538e-04\n",
      "  5.29551739e-03  6.73261732e-02 -6.54115528e-03  7.80281378e-03\n",
      "  2.52764323e-03  3.05105504e-02  8.98415223e-03 -5.13392687e-03\n",
      " -1.20104469e-01 -4.69145291e-02  1.79743220e-03 -1.35557754e-02\n",
      " -5.18565578e-03 -2.15638019e-02 -8.21015239e-02 -3.08913905e-02\n",
      " -3.13958968e-03  1.55976368e-02 -1.86668057e-02  2.17851363e-02\n",
      " -2.78169829e-02  2.23369664e-03  2.61427835e-02  1.23035908e-01\n",
      "  9.41851083e-03 -3.34113203e-02  9.52900213e-04 -1.14723435e-02\n",
      " -2.54388768e-02  1.74711999e-02 -1.27325440e-02  2.34327801e-02\n",
      " -6.50561322e-03 -3.15056294e-02  4.00204957e-02 -1.75204743e-02\n",
      " -3.21819745e-02  8.31385888e-03  3.97440139e-03 -1.52966502e-04\n",
      " -2.80184150e-02  4.70169121e-03  1.46974192e-03 -1.13325650e-02\n",
      " -5.72214834e-03 -5.99587671e-02  5.44262910e-03 -2.47438159e-02\n",
      " -6.62134821e-03  1.02907687e-03  5.68696745e-02  1.23998541e-02\n",
      " -2.32839435e-02 -5.25116771e-02  3.73313986e-02 -5.94004849e-03\n",
      " -7.77726471e-02 -1.93003076e-03 -1.13197416e-02 -2.04954166e-02\n",
      " -8.95634759e-03  3.67158912e-02  1.15514360e-03 -1.19225848e-02\n",
      " -8.81869253e-03 -1.10295825e-02  2.66812965e-02  1.41805736e-02\n",
      "  6.80700392e-02 -9.18679386e-02  2.78964471e-02  8.84323660e-03\n",
      "  7.79901743e-02  5.48136719e-02 -3.67508307e-02  5.50118126e-02\n",
      " -3.00299637e-02 -3.53143439e-02  5.96881770e-02 -3.39656696e-02\n",
      " -1.13096898e-02  4.43609804e-03  2.77203578e-03 -2.02367269e-02\n",
      "  3.51484232e-02  1.13079078e-01 -4.63342369e-02  3.88259962e-02\n",
      " -1.86476819e-02  1.12971114e-02 -5.73386326e-02 -7.41641670e-02\n",
      " -9.45647154e-03  4.54243599e-03  5.55924699e-02 -1.12095140e-02\n",
      " -1.11085467e-01 -6.33016080e-02 -2.54527386e-02  1.68558322e-02\n",
      "  1.55172544e-02  1.64076276e-02  4.97103445e-02  7.23149953e-03\n",
      "  8.70617107e-02 -1.03301434e-02  1.74286310e-02  4.10450138e-02\n",
      " -1.77198164e-02  5.57743795e-02  8.54194760e-02 -3.22293937e-02\n",
      "  6.02553991e-05 -1.82111468e-02  4.18002494e-02 -4.54486255e-03\n",
      "  6.06909255e-03  4.72029336e-02  3.51321287e-02 -4.84455330e-03\n",
      " -8.85336567e-03 -4.43408405e-03  1.60441478e-03 -3.50646228e-02\n",
      "  1.70772728e-02 -1.76425651e-02  4.87591587e-02 -5.64308427e-02\n",
      "  2.70943157e-02  9.37856659e-02  1.90650765e-02 -4.21900600e-02\n",
      "  3.07260361e-02  3.75022180e-02  7.02630058e-02 -2.64965128e-02\n",
      "  1.09423827e-02 -8.16937909e-03 -7.22069293e-02 -1.45505983e-02\n",
      "  1.00732800e-02  2.15294864e-02 -1.19404569e-02  6.15777040e-04\n",
      " -3.28590050e-02  1.23185180e-02  5.20523004e-02 -3.01617682e-02\n",
      " -7.32690329e-03 -5.65031655e-02 -4.21946198e-02 -3.29130031e-02\n",
      "  5.01795970e-02 -2.53276937e-02  4.40251455e-02 -4.39979881e-03\n",
      "  2.82253623e-02 -1.59562360e-02  3.60585116e-02  2.26648971e-02\n",
      "  2.63986234e-02 -4.12636809e-03 -2.01933365e-02  5.60454391e-02\n",
      " -7.75078610e-02 -5.12632094e-02 -5.54347597e-02  3.01038548e-02\n",
      " -7.80607946e-03 -3.61765764e-04 -3.65015119e-02 -2.78929621e-02\n",
      "  1.64858084e-02  3.03925737e-03 -2.06075367e-02  4.35701311e-02\n",
      " -1.78232528e-02 -5.46556376e-02 -5.98938763e-02 -6.10608198e-02\n",
      "  6.46756729e-03  4.40894030e-02  3.64730768e-02  4.72572893e-02\n",
      "  3.86716537e-02 -5.00394031e-02 -2.08262820e-02  2.98219528e-02\n",
      "  3.72984037e-02 -9.00001079e-02  6.22749291e-02  6.98580500e-03\n",
      " -6.07004343e-03 -2.42993217e-02 -9.38536040e-03  4.33174968e-02\n",
      " -1.79818757e-02 -3.64482217e-02 -1.72035582e-02 -8.34737718e-03\n",
      " -1.31905237e-02 -2.95930804e-04  3.85857932e-02 -9.81796999e-04\n",
      " -2.61821039e-02 -7.28517829e-04 -1.06650060e-02  1.78858545e-02\n",
      " -5.98301506e-03  4.11062464e-02 -9.90627799e-03 -1.74736080e-04\n",
      "  2.50294413e-02  6.35222942e-02  1.01676229e-02 -1.86757604e-03\n",
      " -4.39276285e-02  4.25625499e-03 -2.24861875e-02 -5.55245243e-02\n",
      "  6.55782828e-03  5.38155921e-02  5.75981550e-02 -6.84882561e-03\n",
      "  2.97433184e-03  1.43657567e-03 -2.80391779e-02 -4.51723859e-02\n",
      "  9.31319222e-03  3.75874452e-02  1.89428888e-02  3.38583253e-03\n",
      " -8.31136405e-02 -3.62538919e-02  1.80430543e-02  3.00674383e-02\n",
      " -1.07328661e-01 -6.29787371e-02  2.53579486e-02 -9.29859933e-03\n",
      "  6.56657480e-03 -6.20015524e-02 -8.23128223e-03  8.50402378e-03\n",
      " -2.25217510e-02 -2.71083023e-02  2.74793543e-02  1.93629321e-02\n",
      "  4.33571776e-03 -1.10191386e-02  4.87715472e-03  5.14216758e-02\n",
      "  4.71536331e-02  4.86804871e-03  1.91889517e-02  1.08595891e-02\n",
      " -1.00035267e-02  8.19580704e-02  5.31410351e-02 -4.26432565e-02\n",
      "  5.09817153e-04  3.44155878e-02 -3.05133294e-02  1.50111122e-02\n",
      "  2.24637613e-02 -1.90836079e-02 -9.49083362e-03 -1.36181144e-02\n",
      "  6.34553954e-02 -6.76329294e-03  1.62797179e-02  1.73307899e-02\n",
      " -1.11008985e-02  2.87627755e-03  5.11699542e-02 -2.72158720e-02\n",
      " -9.42742731e-03  1.36314938e-03  3.17385048e-02 -4.52426299e-02\n",
      " -9.61744320e-03  6.92484761e-03  8.02225713e-03 -3.26908119e-02\n",
      "  9.90094803e-03  1.32540818e-02 -7.30094034e-03  8.51809233e-03\n",
      " -1.30482751e-03 -1.08315074e-03  3.20030190e-02  3.14891315e-03\n",
      "  7.94534832e-02 -1.79487467e-02  9.97319631e-03 -1.22236216e-03\n",
      " -1.39302816e-02  7.44187832e-02  5.58349886e-04 -1.40934829e-02\n",
      " -1.53853185e-02 -5.09334691e-02  4.46641780e-02 -7.09955096e-02\n",
      " -1.63558349e-02 -8.56206417e-02  1.85677595e-02  3.36691644e-03\n",
      " -1.38039002e-03 -2.95924302e-03 -1.77954789e-02  2.98965313e-02\n",
      "  4.67825271e-02  9.37030651e-03 -5.01471721e-02 -4.45365235e-02\n",
      " -2.96059973e-03 -9.27638821e-03  2.03252863e-02 -1.26807680e-02\n",
      " -1.01856114e-02  3.61068957e-02 -2.54030097e-02 -8.86848569e-02\n",
      " -6.28453270e-02  3.06237880e-02  4.33166362e-02  5.73439188e-02\n",
      "  7.57530658e-03  2.18469594e-02 -7.64158890e-02 -7.38213956e-03\n",
      " -2.85439827e-02  1.04108443e-02 -8.18847585e-03  1.18531361e-02\n",
      "  1.85222160e-02 -4.33787256e-02 -9.30284988e-03  2.44053341e-02\n",
      "  7.35505344e-03  7.37771904e-03  1.11216668e-03  1.91687942e-02\n",
      " -4.16048244e-02  2.49515101e-02  4.07619476e-02 -4.55944724e-02\n",
      "  1.78680494e-02  8.00890103e-03  2.18390916e-02  5.56571111e-02\n",
      " -5.37003111e-03 -4.88528572e-02 -4.12863791e-02  3.19420770e-02\n",
      " -2.54704226e-02 -2.60110646e-02 -5.06797200e-03 -2.79912420e-05\n",
      "  5.73683158e-03  7.29488628e-03 -3.15863006e-02 -3.27526107e-02\n",
      " -2.30454765e-02 -4.34959568e-02  2.86551807e-02 -3.21001559e-02\n",
      " -1.47465756e-02 -3.90741415e-03 -1.27215767e-02 -1.60765015e-02\n",
      "  5.68523556e-02  1.45339621e-02 -5.22086769e-02  4.36322130e-02\n",
      " -4.24222648e-03  3.11892331e-02 -3.44944298e-02  1.36722103e-02\n",
      " -5.37388958e-02 -6.16377965e-02 -3.54708433e-02  5.34805432e-02\n",
      "  4.72623948e-03 -2.69309729e-02 -4.74975035e-02 -3.41986232e-02\n",
      " -5.67312166e-02  5.57270972e-03  1.33736962e-02  1.28663238e-02\n",
      " -3.54090333e-03 -1.49651933e-02  2.37671118e-02 -1.19584175e-02\n",
      "  1.44058047e-02  3.88698801e-02 -8.81305244e-03  1.49019090e-02\n",
      " -1.80756040e-02  3.95486727e-02  3.26493382e-02  1.92336943e-02\n",
      "  2.80066226e-02 -2.25521028e-02  1.01810405e-02  8.80745240e-03\n",
      " -1.19205717e-04  3.16090658e-02  1.57273524e-02 -1.03972657e-02\n",
      " -6.82643673e-04 -7.63202310e-02  1.36960847e-02 -3.44413817e-02\n",
      "  4.67066988e-02  3.15915681e-02 -7.37603158e-02 -4.48245704e-02\n",
      " -5.29918633e-02  5.88925835e-03  4.03932063e-03  1.44532830e-01\n",
      "  2.61326656e-02  9.76925045e-02  1.98891107e-02 -1.07663646e-02\n",
      " -5.94727099e-02 -2.61182971e-02  7.93977380e-02  3.59431803e-02\n",
      " -2.20083501e-02  5.50622493e-02 -9.93732177e-03  3.18289065e-04\n",
      "  4.19274494e-02 -2.13183314e-02 -3.88250649e-02  2.43681855e-02\n",
      " -5.51974811e-02 -9.42408480e-03  5.36436364e-02 -5.06284499e-33\n",
      "  3.40705104e-02  1.69876264e-04  1.59193613e-02  9.17442888e-02\n",
      " -6.93077520e-02  3.37244128e-03 -3.40593867e-02  2.90028453e-02\n",
      " -4.81807031e-02  2.16673240e-02  1.56244226e-02 -2.08207946e-02\n",
      "  2.95062866e-02  5.71270753e-03  1.68322250e-02 -1.68643035e-02\n",
      " -1.57535952e-02 -3.26036662e-03  3.73231694e-02  1.02282129e-02\n",
      " -2.51418687e-02 -2.47167312e-02 -6.67591393e-02  4.91437502e-02\n",
      "  6.79666316e-03 -1.89834423e-02 -1.18214702e-02  4.44414541e-02\n",
      " -4.71669436e-03 -2.16907822e-02 -2.23114365e-03 -2.34781243e-02\n",
      " -8.65652412e-03 -1.16623230e-02 -9.23517998e-03  1.56764314e-02\n",
      " -4.23814282e-02  3.44023742e-02 -5.84521070e-02  2.64999233e-02\n",
      " -2.74190363e-02 -2.25919709e-02  2.58905301e-03 -1.78657640e-02\n",
      " -1.94304492e-02 -6.06352510e-03 -4.87575270e-02 -7.54820835e-03\n",
      "  2.05440614e-02 -4.19459939e-02  5.61738038e-04 -8.45258124e-03\n",
      "  5.05178422e-03  7.15151615e-03  2.12985035e-02  1.39054516e-02\n",
      "  4.89404611e-03  3.11994199e-02  2.91483849e-02 -5.04316725e-02\n",
      " -7.00094476e-02 -3.53463292e-02 -2.65285596e-02  2.91448589e-02\n",
      " -2.62825526e-02 -3.21840052e-03  1.46810621e-01  4.90882341e-03\n",
      "  1.95240751e-02 -2.83215623e-02  1.74101461e-02 -2.94339862e-02\n",
      " -3.05857006e-02  5.88824041e-02 -4.16564867e-02  6.27255440e-03\n",
      " -1.38355896e-03  1.15800872e-02  5.94669301e-03 -4.86366032e-03\n",
      " -6.11484684e-02 -2.03677043e-02  1.79964956e-02  1.63885728e-02\n",
      "  3.21763637e-03  1.06688803e-02 -1.30005563e-02  2.44787093e-02\n",
      "  4.27046716e-02 -2.85113957e-02  4.27567288e-02  3.77678797e-02\n",
      "  1.15708290e-02 -1.33389737e-02  8.58629029e-03  7.15356600e-03\n",
      " -1.67310368e-02  6.23135408e-03  1.00359824e-02 -2.62450725e-02\n",
      " -3.26163247e-02  4.28309664e-02 -1.27257882e-02  1.41982352e-02\n",
      "  1.47050414e-02 -5.16086468e-04 -6.70432821e-02  1.01804445e-02\n",
      " -2.63758749e-02 -7.06704333e-04 -6.76540099e-03  7.02959578e-03\n",
      " -1.33639446e-03 -1.88417267e-02 -2.84316819e-02  2.41801776e-02\n",
      " -1.66141603e-03 -1.92303455e-03 -4.39022407e-02 -3.82206105e-02\n",
      "  1.63314096e-03  2.71879118e-02 -3.78967486e-02  2.36983784e-02\n",
      "  2.10203640e-02 -2.06805742e-03  1.12233683e-02  1.76181681e-02\n",
      " -5.66013157e-02  2.95262486e-02 -4.09061909e-02 -3.27560566e-02\n",
      "  2.07318521e-07  6.08254224e-02  1.94378886e-02 -8.13725516e-02\n",
      " -4.60809544e-02 -1.32105025e-02  4.13957126e-02 -1.72559209e-02\n",
      " -3.10975332e-02  3.60525362e-02  5.04496694e-02  3.44310179e-02\n",
      " -6.61241589e-03 -8.90119839e-03 -3.12849879e-02  3.10944691e-02\n",
      "  3.62673476e-02 -3.61334831e-02 -5.64406961e-02  1.52979707e-02\n",
      " -1.38744004e-02  4.84196544e-02 -3.97456810e-02 -1.64964348e-02\n",
      "  9.39748250e-03 -1.08891204e-02 -4.42709215e-02  5.07502966e-02\n",
      "  3.44792567e-02 -2.96341497e-02 -1.64302308e-02  4.15785797e-02\n",
      " -4.81570102e-02  4.83606458e-02  3.77876940e-03  5.08559542e-03\n",
      " -2.67351475e-02 -1.63014177e-02  7.32295215e-02  2.67139375e-02\n",
      "  5.64373210e-02 -5.11952266e-02  2.07509845e-02 -4.53410335e-02\n",
      "  4.08780500e-02  1.41661987e-02  1.15580996e-02 -1.61986388e-02\n",
      " -1.77985132e-02 -5.76748103e-02  4.05662842e-02 -4.30350080e-02\n",
      "  6.06826618e-02  2.56904159e-02  8.39414224e-02  1.89978839e-03\n",
      "  2.52233073e-02  1.55361583e-02  7.86273624e-04  2.76075932e-03\n",
      " -6.02087379e-02 -5.98988868e-02  8.37798864e-02 -2.09292844e-02\n",
      " -4.87794615e-02  7.82070041e-04  1.80677809e-02  5.89558203e-03\n",
      "  3.23672972e-35 -1.77099071e-02  4.39653099e-02  3.31627652e-02\n",
      "  2.92506982e-02 -5.70926182e-02  6.36313111e-03 -3.12086884e-02\n",
      "  1.50251528e-02 -2.29461864e-02  3.71415634e-03 -4.75074071e-03]\n",
      "Embedding size: (768,)\n"
     ]
    }
   ],
   "source": [
    "single_sentence = \"Who is Michael Jordan?\"\n",
    "single_embedding = embedding_model.encode(single_sentence)\n",
    "print(f\"Sentence: {single_sentence}\")\n",
    "print(f\"Embedding:\\n{single_embedding}\")\n",
    "print(f\"Embedding size: {single_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4491ef3",
   "metadata": {},
   "source": [
    "Nice! We've now got a way to numerically represent each of our chunks.\n",
    "\n",
    "Our embedding has a shape of `(768,)` meaning it's a vector of 768 numbers which represent our text in high-dimensional space, too many for a human to comprehend but machines love high-dimensional space.\n",
    "\n",
    "How about we add an embedding field to each of our chunk items?\n",
    "\n",
    "Let's start by trying to create embeddings on the CPU, we'll time it with the `%%time` magic to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ac3ac47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ef040df48044ffa116dade22e36e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 38s, sys: 8.17 s, total: 11min 47s\n",
      "Wall time: 12min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# the model is on the CPU\n",
    "embedding_model.to(\"cpu\")\n",
    "\n",
    "# Embed each chunk one by one\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f02aa2",
   "metadata": {},
   "source": [
    "This would take a *really* long time if we had a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d63a324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 524 μs, sys: 0 ns, total: 524 μs\n",
      "Wall time: 570 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'often. • Calm your “sweet tooth” by eating fruits, such as berries or an apple. • Replace sugary soft drinks with seltzer water, tea, or a small amount of 100 percent fruit juice added to water or soda water. The Food Industry: Functional Attributes of Carbohydrates and the Use of Sugar Substitutes In the food industry, both fast-releasing and slow-releasing carbohydrates are utilized to give foods a wide spectrum of functional attributes, including increased sweetness, viscosity, bulk, coating ability, solubility, consistency, texture, body, and browning capacity. The differences in chemical structure between the different carbohydrates confer their varied functional uses in foods. Starches, gums, and pectins are used as thickening agents in making jam, cakes, cookies, noodles, canned products, imitation cheeses, and a variety of other foods. Molecular gastronomists use slow- releasing carbohydrates, such as alginate, to give shape and texture to their fascinating food creations. Adding fiber to foods increases bulk. Simple sugars are used not only for adding sweetness, but also to add texture, consistency, and browning. In ice cream, the combination of sucrose and corn syrup imparts sweetness as well as a glossy appearance and smooth texture.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_chunks=[item['sentence_chunk'] for item in pages_and_chunks_over_min_token_len]\n",
    "text_chunks[419]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd5e4524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1680"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96d4923a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m#Embed all text in batches\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mtext_chunk_embeddings=embedding_model.encode(text_chunks, batch_size=32, convert_to_tensor=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mtext_chunk_embeddings\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2547\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2545\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2546\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2547\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2550\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/IPython/core/magics/execution.py:1390\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1388\u001b[39m st = clock2()\n\u001b[32m   1389\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1391\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1392\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1052\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1049\u001b[39m features.update(extra_features)\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1054\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1133\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1127\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1128\u001b[39m         module_kwargs = {\n\u001b[32m   1129\u001b[39m             key: value\n\u001b[32m   1130\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1131\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1132\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:437\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    431\u001b[39m trans_features = {\n\u001b[32m    432\u001b[39m     key: value\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    435\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    439\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:482\u001b[39m, in \u001b[36mMPNetModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    481\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    491\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:334\u001b[39m, in \u001b[36mMPNetEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    332\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:304\u001b[39m, in \u001b[36mMPNetLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    301\u001b[39m outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n\u001b[32m    303\u001b[39m intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:271\u001b[39m, in \u001b[36mMPNetOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    273\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Embed all text in batches\n",
    "text_chunk_embeddings=embedding_model.encode(text_chunks, batch_size=32, convert_to_tensor=True)\n",
    "text_chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b5364",
   "metadata": {},
   "source": [
    "### Save embeddings to file\n",
    "\n",
    "Since creating embeddings can be a timely process (not so much for our case but it can be for more larger datasets), let's turn our `pages_and_chunks_over_min_token_len` list of dictionaries into a DataFrame and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3340978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to file\n",
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d2b9f",
   "metadata": {},
   "source": [
    "And we can make sure it imports nicely by loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "447c4c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-39</td>\n",
       "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
       "      <td>308</td>\n",
       "      <td>42</td>\n",
       "      <td>77.00</td>\n",
       "      <td>[ 6.74242601e-02  9.02280062e-02 -5.09550702e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-38</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>52.50</td>\n",
       "      <td>[ 5.52156679e-02  5.92137799e-02 -1.66167784e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-37</td>\n",
       "      <td>Contents Preface University of Hawai‘i at Māno...</td>\n",
       "      <td>766</td>\n",
       "      <td>116</td>\n",
       "      <td>191.50</td>\n",
       "      <td>[ 2.79801488e-02  3.39813270e-02 -2.06427258e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-36</td>\n",
       "      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n",
       "      <td>941</td>\n",
       "      <td>144</td>\n",
       "      <td>235.25</td>\n",
       "      <td>[ 6.82566613e-02  3.81274521e-02 -8.46859254e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35</td>\n",
       "      <td>The Cardiovascular System University of Hawai‘...</td>\n",
       "      <td>998</td>\n",
       "      <td>152</td>\n",
       "      <td>249.50</td>\n",
       "      <td>[ 3.30264419e-02 -8.49772710e-03  9.57151968e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n",
       "1          -38  Human Nutrition: 2020 Edition by University of...   \n",
       "2          -37  Contents Preface University of Hawai‘i at Māno...   \n",
       "3          -36  Lifestyles and Nutrition University of Hawai‘i...   \n",
       "4          -35  The Cardiovascular System University of Hawai‘...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               308                42              77.00   \n",
       "1               210                30              52.50   \n",
       "2               766               116             191.50   \n",
       "3               941               144             235.25   \n",
       "4               998               152             249.50   \n",
       "\n",
       "                                           embedding  \n",
       "0  [ 6.74242601e-02  9.02280062e-02 -5.09550702e-...  \n",
       "1  [ 5.52156679e-02  5.92137799e-02 -1.66167784e-...  \n",
       "2  [ 2.79801488e-02  3.39813270e-02 -2.06427258e-...  \n",
       "3  [ 6.82566613e-02  3.81274521e-02 -8.46859254e-...  \n",
       "4  [ 3.30264419e-02 -8.49772710e-03  9.57151968e-...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import saved file and view\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd1795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
